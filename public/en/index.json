[
{
	"uri": "http://localhost:1313/aws-report-fcj/en/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "The Internet of Things on AWS \u0026ndash; Official Blog Sharing a vision of a more connected world with AWS IoT by Rachel Burke-Smalley on 03 MAY 2023 in Amazon SageMaker, Automotive, AWS IoT Core, AWS IoT ExpressLink, AWS IoT FleetWise, AWS IoT Greengrass, AWS IoT SiteWise, AWS IoT TwinMaker, AWS RoboMaker, CPG, Energy (Oil \u0026amp; Gas), Internet of Things, Kinesis Video Streams, Manufacturing, Robotics, Thought Leadership\nAWS IoT VP, Yasser Alsaied, shares IoT strategy, commitment, and vision Introduction\nThe future of the Internet of Things (IoT) is a subject of constant discussion and speculation. With the implications for IoT hyperscalers, vendors, and customers, the landscape is shifting rapidly. To shed some light on this topic, I sat down with Yasser Alsaied, Vice President of IoT at Amazon Web Services (AWS). In this blog, we explore Yasser’s perspectives on the future of IoT technology, strategy, industry evolution, and how these factors will impact the IoT ecosystem.\nYasser, thank you for agreeing to share your insights with us today. Could you start by telling us a bit about your role at AWS and your experience in the IoT space? In total, I have about 32 years of experience in the technology and IoT industry. I joined AWS in 2021 as the Vice President of IoT. I lead the AWS IoT business, covering Robotics, Industrial, Automotive, Consumer, Public Sector, and Commercial segments. Our services are among the most highly connected IoT services globally and continue to grow in the areas of digital twins, smart cities, and connected vehicles.\nBefore joining AWS, I worked at Qualcomm as the Vice President of IoT. During my time at Qualcomm, I held several leadership positions, including the launch of Qualcomm’s first wireless local area network (WLAN) chip for mobile phones, the Qualcomm Innovation Center, and the Code Aurora Foundation to address legal and operational issues related to open-source software releases. In my last four years at Qualcomm, I managed their IoT ecosystem and led the strategy for expanding IoT chipsets, including key technologies such as artificial intelligence (AI), computer vision, drones, robotics, and 5G. Prior to Qualcomm, I developed software for military applications, working with the U.S. Space and Rocket Center, Raytheon, Lockheed Martin Missiles and Fire Control, and Coleman Aerospace.\nHaving been involved since the early days of IoT, what are your thoughts on the current changes in the IoT industry, and what does this mean for customers and service providers? As the IoT industry evolves, we continue to see very strong interest and adoption of IoT from customers. We do not anticipate IoT growth slowing down anytime soon. However, it is clear that the role of IoT hyperscalers is changing, and much of this is tied to the industry\u0026rsquo;s shift toward verticalized solutions rather than providing IoT as a horizontal offering.\nThis shift is in the best interest of the customer because they never come to us saying \u0026ldquo;we need IoT.\u0026rdquo; Instead, they come to us wanting to achieve specific business outcomes and need support in defining how they can leverage technology to monitor, control, and optimize business processes to achieve those results. What this shift entails for cloud providers and IoT software vendors is that they must place IoT within the context of distinct industry challenges by providing industry-focused solutions and partners to drive business value based on the customer\u0026rsquo;s internal capabilities, IT scaling requirements, and use cases.\nHow does this shift in roles affect AWS\u0026rsquo;s strategy?\nAt AWS, we have adopted an industry-led approach as the \u0026ldquo;North Star\u0026rdquo; of our IoT strategy. To better serve customers, we have verticalized our IoT and AI services to optimize outcomes for each segment. For example, [AWS IoT SiteWise] and [AWS IoT TwinMaker] are built to support Industrial IoT customers in collecting, organizing, and analyzing industrial data, as well as creating digital twins of real-world industrial environments. [AWS IoT FleetWise] supports Automotive and connected vehicle customers. We also collaborate with industry-focused partners to build solutions. Our principle is to re-architect IoT products, sales, and support to align with each industry, thereby meeting customer needs more effectively.\nWhy are vertical industry solutions and partners so important to AWS and its customers? Some customers, when starting with IoT, face difficulties in moving from proof of concept to production, scaling up, and integrating all the necessary services. There are too many components to combine in a workload, and customers don\u0026rsquo;t care which services are included or how they are integrated. Instead, they just want industry-focused solutions that are placed within the context of specific challenges and are easy to manage, adapt, expand, and replicate.\nTherefore, a key part of our IoT strategy is focusing on vertical AWS and Partner Solutions, providing ready-to-deploy code, configurations, and architectural guidance customizable for specific use cases. Our approach always starts with understanding the customer\u0026rsquo;s unique vision and challenges, such as improving yield or fleet management. From there, we work backward, designing a solution tailored to their specific needs.\nUsing our trusted and scalable cloud platform, with over 200 services including AI, machine learning, analytics, compute, networking, and storage, we enable the building of solutions that combine all necessary elements. When something new needs to be built on this platform, we leverage our partner network to create solutions suitable for individual customers, which can also benefit other customers in the same industry. This service formula can be replicated globally, accelerating the process.\nAnother key initiative of ours is to continue developing and collaborating with the [AWS Partner Network] (APN), because we understand that customer access to a diverse range of partners is critical to filling skill gaps, meeting scaling requirements, and specializing in vertical use cases. These are partners who can support various stages of the IoT design lifecycle. ISV partners provide pre-built software solutions that can be integrated on top of our platform and IoT services to enhance capabilities and features. When customers need customization to meet their exact requirements, they can work with a highly specialized SI to help integrate various systems and technologies into a unified solution without needing to invest in additional internal engineering resources.\nBeyond pursuing vertical solutions, what does the future of IoT at AWS look like? Where is AWS placing its big bets?\nThere has been a lot of speculation about what we will do next, and I can confirm that we remain strongly committed to continuously innovating and making it easier and more cost-effective for customers to realize IoT solutions. In the two years since I joined AWS, we have launched new specialized services such as AWS IoT FleetWise, [AWS IoT ExpressLink], and AWS IoT TwinMaker. Our inventory of qualified devices has grown to over [800 partner devices]. We have announced [price reductions] to drive cost efficiency and deployed over [50 IoT feature updates], including the newly announced [AWS IoT Core for Amazon Sidewalk], which enhances the [integration] between [AWS IoT Core] and Amazon Sidewalk. We will not stop. We will continue to invest heavily in IoT because it is an integral component of how we solve customer problems and help them scale. Use cases like Industry 4.0 require data to be managed and processed at scale, in a reliable and secure manner. The data is already there, but without IoT, it is locked in isolated factories and production floors. IoT facilitates connectivity and unlocks data, making smart factories a reality. IoT is an essential part of the digitalization story – whether it\u0026rsquo;s 5G, Wi-Fi 7, LPWAN, or other technologies.\nFurthermore, our commitment to making IoT solutions more accessible is directly tied to our ongoing efforts to expand edge computing and hybrid cloud capabilities. By extending AWS infrastructure, services, APIs, and tools to edge locations like on-premises data centers, 5G towers, and smart factories, we can bring all the benefits of the cloud to workloads requiring low latency, local data residency, local data processing, or complex dependencies between applications. Today, AWS offers IoT services like [AWS IoT SiteWise Edge] and [AWS IoT Greengrass]{.underline} that act as edge gateways, making it easy to collect, process, and monitor device data on-site while bringing intelligence to edge devices. AWS is also developing functions to make deploying applications at the edge easier, shortening the time to value for customers and shifting capital expenditure (CapEx) to operating expenditure (OpEx).\nWhy is IoT important to Amazon\u0026rsquo;s success?\nMany of Amazon\u0026rsquo;s core business areas, such as warehousing operations, fulfillment centers, transportation, logistics, stores, and devices, are directly supported by AWS IoT services. We are also our own customer — and we are always customer-obsessed — so the service must be the best. Through our vast customer base, including Amazon, this technology is proven in reliability and scalability. We are stronger together and driven by core operational needs as well as Amazon\u0026rsquo;s DNA of innovation.\nFor example, IoT plays a critical role in the retail logistics chain and shopping process. Globally, Amazon has over 300 fulfillment centers operated by IoT and edge devices such as over 750,000 autonomous robotic units. This scale supports 1.6 million packages passing through their fulfillment centers every day. [Automation and robotics are a big part of this equation], and these technologies require real-time control, monitoring, and maintenance to keep Amazon\u0026rsquo;s supply chain running like clockwork. With AWS IoT, Amazon can collect and process telemetry data that helps operators monitor their activities, track critical maintenance schedules, and ensure all packages are delivered on time.\nAmazon also operates a large fleet of trucks for pickups across the country. previously, at many Amazon Fulfillment Centers, drivers had to wait in long lines to check in and find out which dock they were assigned to. But with AWS IoT, and specifically the specialized service [Amazon Kinesis Video Streams], we streamlined the check-in and check-out process. Now, drivers can automatically check in and see where they need to go right on their mobile app. This saved 775,000 hours for drivers and carriers across North America in 2022 alone. Additionally, Amazon has the Amazon Flex app, which coordinates field deliveries for Amazon delivery personnel. Flex sends and receives data to dozens of backend services via AWS IoT, processing up to 22,000 transactions per second to coordinate deliveries. Finally, when you shop at physical retail stores, many Amazon and third-party stores use Just Walk Out technology to skip the checkout line. To enter the store, customers can use the Amazon Shopping App, Amazon One, or a credit card, simply take the products they want, and leave, making shopping faster and more convenient. I have experienced this myself and I am very impressed with the convenience. Just Walk Out uses Kinesis Video Streams to transmit video from in-store cameras to AWS, running advanced computer vision algorithms that allow shoppers to enter the store, grab what they want, and leave.\nBeing our own customer also brings its own challenges. The massive scale at which Amazon operates globally means AWS IoT must support devices, telemetry data streams, and security at the same scale. Our internal deployments continue to yield new lessons that help the service become more robust, efficient, and cost-effective for customers to benefit from.\nWhich industries are benefiting from AWS IoT? Over the past decade, IoT has evolved from an aspirational technology to a core differentiator that businesses are using to solve problems and create value through revenue growth, operational improvements, and innovation. For consumers, we partner with many Original Equipment Manufacturers (OEMs) and Original Design Manufacturers (ODMs) with connected devices. They can quickly build secure products, connect devices, and support scalability and reliability. One example is Rachio, which uses AWS IoT to securely connect its devices to cloud applications and other devices. AWS IoT also gave [Rachio] a seamless gateway to other AWS services, such as [AWS Elastic Beanstalk] for deploying and managing their website, web apps, and API infrastructure. Most recently, Rachio began using the Amazon Alexa Skills Kit to enable voice control for the second version of the Rachio Smart Sprinkler Controller. With AWS IoT security capabilities, Rachio reduced development costs by 40% and no longer has to worry about managing availability and scalability.\nIn industrial and manufacturing, we partner with customers to improve process efficiency and yield, track inventory, manage operations, and safety. One example is our customer, [Coca-Cola İçecek (CCI)], who used AWS IoT to digitize their factory floor and production processes to create a complete digital twin solution scalable to all 26 bottling plants. Within two months, they built an advanced digital analytics solution for their line sanitation process using AWS IoT SiteWise and AWS IoT Greengrass. Through this, they improved process efficiency and environmental sustainability, allowing for savings of 20% in energy and 9% in water annually. In the public sector, smart cities, and transportation, customers are using IoT to improve operations, logistics, tactical edge support, and traffic management as well as public safety.\nIn commercial and smart buildings, customers are looking to improve retail experiences and enhance building sustainability. Finally, in Robotics, customers are using IoT services to collect data from robots to build new applications, optimize automation, and improve performance. For connected vehicles, IoT helps customers manage vehicle data, thereby improving machine learning models and in-cabin experiences. For example: [WirelessCar] is partnering with AWS IoT to manage their fleet at scale without managing infrastructure, allowing them to connect millions of vehicles to the cloud.\nWhat IoT trends should we expect as we move into the next phase? I believe IoT will become a common expectation in business rather than an exception. Many businesses and industries will continue to invest in IoT because it delivers business and operational value. We continuously see new customer segments, like the retail industry, unlocking the value of IoT. Customers are trying to build a unified experience that can move easily between online and offline, leading to a convergence with mobile, social, and IoT to serve wherever and whenever they desire. We also see increasing demand for simpler IoT-focused tools. It is important to make these tools more accessible so companies can innovate and leverage them, while easily penetrating industries as the adoption of IoT technologies continues to grow. For example, tools like [AWS IoT Core Device Advisor] help developers validate IoT devices for reliable and secure connectivity with AWS. They can identify device software issues, such as inability to reconnect, and get detailed logs to troubleshoot during development and testing.\nFinally, customers are expanding their sustainability initiatives, going beyond just reducing emissions to creating smart environments (e.g., cities, buildings, factories), leveraging IoT to monitor energy performance, minimize waste, and adjust facility operations based on actual usage trends. A great example of this is how Yara partnered with us to build an efficient and sustainable [Digital Production Platform] (DPP) for the agriculture industry. This DPP platform is a key element in digitizing their production system across 28 production sites, 122 production units, and two mines. The DPP detects, collects, and runs deep analytics on production data related to yield, reliability, environment, safety, quality, and innovation, using AWS IoT SiteWise, AWS IoT Greengrass, AWS IoT Core, [AWS IoT Analytics], and [Amazon SageMaker]. This solution helped Yara predict product quality and composition, improve utility balance at facilities, and detect when machines need repair or maintenance to maintain optimal production efficiency levels.\nYasser, thanks again for giving us an exclusive look at the future of IoT at AWS. Clearly, AWS remains deeply invested in IoT solutions to meet the unique needs of customers, including Amazon itself.\n"
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "New and updated courses from AWS Training and Certification in March 2023 By Training and Certification Blog Editor, March 30, 2023\n(Topics: .NET, Amazon Aurora, Amazon CloudWatch, Amazon Detective, Amazon DynamoDB, Amazon Elastic Container Service, Amazon GuardDuty, Amazon RDS, Amazon Redshift, Amazon Security Lake, Amazon Textract, Amazon VPC, Analytics, Announcements, AWS IoT Device Management, AWS Lambda, AWS Secrets Manager, AWS Security Hub, AWS Training and Certification, Compute, Culture and Training, Customer Solutions, Database, Developer Tools, DevOps, Enterprise Strategy, Internet of Things, Mainframe Migration, Migration, Migration Solutions, MySQL compatible, Networking \u0026amp; Content Delivery, RDS for MySQL, Robotics, Security, Security, Identity, \u0026amp; Compliance)\nIn March 2023, AWS Training and Certification released 27 digital products, including 14 AWS Builder Labs, one AWS Jam Journey (analytics), two courses for AWS Partners, three courses for c-suite leaders, and one in-person classroom course on security best practices on AWS, to equip individuals with skills to work with AWS services and solutions.\nExam labs have been temporarily removed from the AWS Certified SysOps Administrator – Associate exam. To help organizations expand their ability to train their teams, the AWS Skill Builder Team subscription is now available in 50 countries. New for Learners AWS Skill Builder is our online learning center for all learners, offering over 600+ free courses, along with 125+ AWS Builder Labs, AWS Jam challenges, AWS Cloud Quest and AWS Industry Quest (experiential learning), and AWS Certification preparation resources, available with Individual and Team subscriptions.\nFree Digital Courses on AWS Skill Builder Fundamental Introduction to Robotics on AWS (30 minutes): Designed for technical teams using AWS for robotics development. In this course, you will learn how to use AWS to accelerate robot development and solve common challenges for robotics companies. Securely Connecting AWS IoT Devices to the Cloud (60 minutes): Designed for technical staff and teaches how to securely connect devices to the cloud using AWS IoT Core, ensuring the correct setup of AWS IoT policies. .NET Workloads on AWS App Runner (2 hours): Designed for developers learning how to write code and deploy containerized applications on AWS using C# and Visual Studio. You will learn how to build and test C# code in containers, store and deploy images to the AWS Cloud using AWS App Runner, and how to build and operate scalable containerized workloads on the AWS Cloud. .NET Workloads on AWS Lambda (3 hours): Designed for application developers new to AWS serverless compute, teaching you how to create, deploy, and manage AWS Lambda functions in current and future applications. Handling AWS IoT Device Data and States (2 hours): Designed for technical staff using AWS to handle Internet of Things (IoT) data. You will learn about telemetry and IoT communication protocols, as well as how to use IoT device data in the AWS Cloud. Intermediate Using AWS Solutions – AWS Cloud Migration Factory (90 minutes): Designed for technical individuals, such as solutions architects and server administrators. This course teaches the key features, use cases, components, and architecture of the AWS Cloud Migration Factory, helping you plan and execute large-scale server migrations. Advanced Planning Large-Scale Data Migrations to AWS (60 minutes): Designed for technical individuals, such as storage engineers and cloud architects. This course teaches key concepts, considerations, and requirements for performing large-scale data migrations to AWS, as well as how to create a data migration plan using the appropriate AWS storage service. Training for AWS Partners Intermediate AWS Partner: Mainframe Modernization Proof of Concept Scoping with AWS Blu Insights (Technical) (70 minutes): Designed for solutions architects and project managers at AWS Partner organizations who participate in evaluating mainframe refactoring projects. You will learn how to scope a Proof of Concept (POC) to transform legacy mainframe application source code and how to define POC boundaries using AWS Blu Insights. AWS Partner: AWS Mainframe Modernization Automated Refactor Transformation Center (Technical) (75 minutes): Designed for solutions architects at AWS Partner organizations who gather modernization requirements. You will learn how to transform and refactor legacy mainframe applications from COBOL to Java via AWS Blu Insights, and how to use the AWS Mainframe Modernization Automated Refactor Transformation Center to handle code errors, generate code refactoring, and deploy custom transformations. AWS Certification As of March 28, 2023, the AWS Certified SysOps Administrator – Associate exam will not include exam labs until further notice. This removal is temporary while we evaluate the exam labs and make improvements to provide the best experience for candidates. If you are preparing for this exam, please use the resources listed on the exam page. New Features for AWS Skill Builder Subscriptions Engage in the Best of re:Invent Analytics 2022: An AWS Jam Journey containing analytics challenges introduced at AWS re:Invent 2022, designed for anyone interested in leveraging analytics skills to complete hands-on challenges in a gamified format. This AWS Jam Journey joins other learning paths on Security, Networking, Machine Learning, Database, Games, and DevOps. AWS Builder Labs AWS Builder Labs are guided, self-paced, interactive learning environments in a live AWS Management Console. We have added 14 new labs to the collection of over 125 existing labs.\nFundamental Getting Started with Amazon Textract: Process Documents with Synchronous and Asynchronous Operations (60 minutes): Designed for developers, data engineers, and machine learning engineers. This lab guides you through navigating the pre-built AWS Cloud9 environment, reviewing AWS CDK project files and Python packages. You will test a synchronous process (extracting a 1-page document) and an asynchronous process (multi-page document). Build, Secure, and Monitor Networks on AWS (2 hours): Designed for DevOps and infrastructure engineers. You will help \u0026ldquo;AnyCompany\u0026rdquo; scale and secure their AWS network environment. Working with Amazon VPC Network Access Analyzer (60 minutes): Designed for system administrators. You will learn how to use Amazon VPC Network Access Analyzer to understand network configurations and identify unintended access. Walkthrough of the AWS Well-Architected Tool (30 minutes): Designed for solutions architects and cloud engineers. Teaches how to use the Well-Architected Tool to create/update a workload and review reports. Intermediate Zero Trust Architecture for Service-to-Service Workload (75 minutes): Designed for system administrators, teaching skills to deploy and verify security controls according to zero trust principles. Building with Amazon Redshift Clusters (60 minutes): Designed for solutions/database architects. Use the AWS Management Console and SQL Workbench to experiment with table layouts and schema designs. .NET Workloads on AWS Lambda (60 minutes): Designed for .NET developers. Learn how to deploy, edit, and invoke serverless .NET applications using the AWS Toolkit for Visual Studio. Migrating RDS MySQL to Aurora with Read Replica (60 minutes): Perform a data migration process from MySQL to Aurora with step-by-step guidance. Building Highly Available Web Application (2 hours): Learn how to set up a highly available three-tier web application using core AWS services. Building with Amazon RDS Databases (60 minutes): Learn how to set up Amazon RDS Multi-AZ failover with encryption and implement AWS Secrets Manager]. Building with Amazon Aurora Databases (60 minutes): Explore parallel query, how to scale with large datasets, and automated triggering scenarios. Building with Amazon DocumentDB Databases (60 minutes): Learn a method to convert a relational data model to non-relational and import data into Amazon DocumentDB. Building with Amazon DynamoDB Tables (60 minutes): Learn how to manage DynamoDB, run queries, backup, monitor, and use Amazon DAX clusters. Advanced Observing, Troubleshooting, and Optimizing Workloads Running on Amazon ECS (90 minutes): Designed for solutions architects and developers. Learn how to deploy, maintain, and troubleshoot applications on an Amazon ECS container cluster. New for Executives, Teams, and Organizations Free Digital Training, Fundamental Cloud for CFOs (13 minutes): Designed for Chief Financial Officers. Learn what cloud computing is, its role in transformation, challenges, and how to partner with CIOs. Cloud for CIOs (9 minutes): Designed for Chief Information Officers. Learn why you should move to the cloud, how to succeed, and how to get started. Digital Transformation for Executives (7 minutes): Designed for executive leaders. Learn about digital transformation, its importance, and how to implement it. AWS Classroom Training Security Engineering on AWS: An intermediate 3-day training, instructor-led. Designed for security engineers/architects and cloud architects. Includes 8 modules and 7 new hands-on labs covering Amazon Security Lake, AWS GuardDuty, Amazon Detective, and more. AWS Skill Builder Team Subscription – Global Expansion AWS Skill Builder Team subscription is now available in 50 countries. New countries include: Argentina, Austria, Bahrain, Belgium, Bulgaria, Chile, Cyprus, Czech Republic, Estonia, Finland, Iceland, Indonesia, Lithuania, Luxembourg, Malta, Mainland China, Mexico, Norway, Poland, Portugal, Romania, Slovakia, Slovenia, South Africa, Sri Lanka, Sweden, and Ukraine.\n"
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Hannover Messe: Discover an end-to-end industrial data strategy with AWS by Scot Wlodarczak on 20 MAR 2023 in Agriculture, Amazon EC2, Amazon Lookout for Vision, Amazon Monitron, Amazon SageMaker, Amazon Textract, Automotive, AWS IoT Core, AWS IoT FleetWise, AWS IoT Greengrass, AWS IoT SiteWise, AWS Outposts, AWS RoboMaker, AWS Snow Family, CPG, Edge, End User Computing, Energy, Energy (Oil \u0026amp; Gas), Events, Hi-Tech and Electronics, Life Sciences, Manufacturing, Power \u0026amp; Utilities, Semiconductor, Supply Chain, Sustainability, Telecommunications, Thought Leadership\nVisit our booth The world’s largest industrial conference, Hannover Messe, begins on April 17, 2023 in Hannover, Germany. Once again, look for Amazon Web Services (AWS) to have a large presence at the event, but in a different location than previous years, in Hall 15 booth D74 right by the main entryway.\nTo get your free code to attend the event, check out our event website page here, or click here to setup a 1:1 meeting with an AWS expert.\nIndustrial Data Fabric (IDF) Data is the foundation of any digital transformation effort, and having an industrial data strategy is critical to enable business teams to easily and effectively leverage that data to address a variety of use cases across an organization.\nWhy? Manufacturers have struggled with disconnected and siloed data sources that were not designed to work together, making it challenging to solve business problems and make informed decisions with non-homogenous data. While they might find success with a single proof of concept on a specific use-case, they often find challenges with scaling implementation to other factories, other use cases, and different teams.\nAWS helps create a data management architecture that enables a scalable, unified, and integrated mechanism to harness data as an asset. We call this the Industrial Data Fabric (IDF).\nIDF Benefits: By providing economical, secure, structured, and easy access to high quality datasets across an entire organization, business leaders can build the foundation for digital industrial transformation and operational optimization across multiple scenarios beyond a single proof-of-concept such as in quality, maintenance, material management, and process optimization. IDF Solutions: Include Open Industrial Data Architecture that enables best design practices for storage and easy accessibility of industrial data, with interoperability between AWS Services and AWS Partner Solutions so manufacturers aren’t locked into proprietary data schemas or creating more data silos in the cloud. We implement IDF in multiple ways, with AWS Solutions, prescriptive solution guidance such as reference architectures, partner solutions like HighByte, Element, General Electric, Mendix Multi-App Platform Package, and AWS Services.\nAn example of Solution Guidance for Industrial Data Fabric (IDF) is the ‘Industrial Data’ AWS Well-Architected Lens with architectural guidance and best practices for AWS Services and partner solutions, enabling manufacturers to accelerate and scale the ingestion, contextualization, and action on industrial data as well as enterprise data across the entire value chain.\nCheck out some of the IDF solutions on the solution page at here. Come to our booth and ask how you can leverage your valuable operational technology (OT) data.\nConnect with Partners Speak with AWS experts at kiosks and demos to learn how cloud and AWS partner solutions help industrial companies in Engineering \u0026amp; Design, Smart Manufacturing, Supply Chain, Sustainability, Smart Products \u0026amp; Services to shorten time to market, improve efficiency, reduce costs, and increase revenue.\nSpeak with platinum sponsors in the booth, including Bosch, Siemens, and MHP, to learn how they can simplify your Industry 4.0 transformation, shorten time to value, and de-risk projects.\nBosch The Bosch Group is a leading global supplier of technology and services. Bosch’s operations are divided into four business sectors: Mobility Solutions, Industrial Technology, Consumer Goods, and Energy and Building Technology. As a leading IoT provider, Bosch offers innovative solutions for smart homes, Industry 4.0, and connected mobility. Bosch is pursuing a vision of mobility that is sustainable, safe, and exciting. Experience their demos and displays at the AWS booth. Siemens Siemens Digital Industries Software helps organizations digitally transform using software, hardware, and services from the Siemens Xcelerator business platform. Siemens’ software and digital twin enable companies to optimize their design, engineering, and manufacturing processes to turn today’s ideas into the sustainable products of the future. Mendix, a Siemens business and global leader in enterprise low-code, is fundamentally reinventing the way applications are built in the digital enterprise. Experience their demos and displays at the AWS booth. MHP As a technology and business partner, MHP digitizes its customers\u0026rsquo; processes and products, and guides them through IT transformations along the entire value chain. MHP is a digitalization pioneer in the mobility and manufacturing sectors, with expertise that can be transferred to a wide range of industries. MHP advises on both operational and strategic issues, offering proven IT and technology expertise as well as specific industry knowledge. Experience their demos and displays at the AWS booth. Other Partners Additionally, there is a range of other AWS Partners also participating in the AWS booth to solve multiple customer use cases. Gold sponsors in our booth include: 42Q, Accenture, ATOS, BCG, Denali, Matterport, along with silver sponsors: ABB, AllCloud, Autodesk, Azeti, Cyient, DXC, Element Analytics, GE Digital, Highbyte, Inductive Automation, Infosys, Mindcurv, Software AG, Software Defined Automation, Storm Reply, Syntax, TensorIT, Tulip, Wipro, and Zoi.\nSee our solutions in action Don’t miss the interactive demos at the booth.\nAWS IoT Partner Device Wall: Shows how partner devices can simplify data connectivity and accelerate digital transformation in the industry. Integrated Manufacturing Value Chain: Watch a demo illustrating how the Industrial Data Fabric is used to build a smart product (a car). From 3D CAD drawings and simulations in the Design \u0026amp; Engineering phase, to a smart factory demo in the Production and Asset Optimization area, and finally seeing the completed car on the track where it collects and visualizes telemetry data in real-time – you will see how AWS and partners collaborate to optimize operations. Sustainability: Dive deeper into the topic of sustainability with the Carbon Footprint solution that helps track your carbon footprint. Supply Chain: With today\u0026rsquo;s trend focusing on smart supply chains, be sure to find out how the AWS Supply Chain service can simplify supply chain needs for complex manufacturing operations. Discover new topics in the theater area The Theater area right in our booth will host more than 50 presentation sessions throughout the week, presented by AWS experts, AWS Partners, and customers. These sessions dive into smart manufacturing, quality management, and industrial automation, allowing you to research deeper and ask more technical questions.\nJoin these short presentations to learn new topics, ask questions, and connect with peers. See the detailed mini-theater schedule posted here and around the AWS booth every day.\nCome to the booth, speak with experts, and see how AWS and partners can help your organization build a data strategy to digitally transform faster, and reduce risk.\nGet a free event discount code to attend here, or click here to book a 1:1 meeting with an AWS expert.\n"
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: “Vietnam Cloud Day 2025: Ho Chi Minh Connect Edition for Builders” Event Objectives Share real-world examples of cloud adoption in enterprises. Highlight the practical business benefits enabled by cloud transformation. Explore best practices, tools, and innovations from AWS and industry leaders. Opening \u0026amp; Keynote Speakers Hon. Government Speaker Eric Yeo, Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Dr. Jens Lottner, CEO, Techcombank Ms. Trang Phung, CEO \u0026amp; Co-Founder, U2U Network Jaime Valles, Vice President, General Manager Asia Pacific and Japan, AWS Jeff Johnson, Managing Director, ASEAN, AWS Vu Van, Co-Founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh, Chairman, Nexttech Group Dieter Botha, CEO, TymeX Track Sessions \u0026amp; Featured Speakers Completing Large-Scale Migration and Modernization with AWS\nSon Do, Technical Account Manager, AWS Nguyen Van Hai, Director of Software Engineering, Techcombank Modernizing Applications with Generative AI-Powered Tools\nPhuc Nguyen, Solutions Architect, AWS Alex Tran, AI Director, OCB Panel Discussion: Application Modernization – Accelerating Business Transformation\nModerator: Hung Nguyen Gia, Head of Solutions Architect, AWS Panelists: Nguyen Minh Ngan, AI Specialist, OCB Nguyen Manh Tuyen, Head of Data Application, LPBank Securities Vinh Nguyen, Co-Founder \u0026amp; CTO, Ninety Eight Transforming VMware with AI-driven Cloud Modernization\nHung Hoang, Customer Solutions Manager, AWS AWS Security at Scale: From Development to Production\nTaiki Dang, Solutions Architect, AWS Key Takeaways AWS continues to be a strong enabler of digital transformation across industries in Vietnam. Enterprises like Techcombank and OCB are actively leveraging AWS for modernization and AI-driven innovation. Generative AI (Amazon Q Developer) is reshaping the software development lifecycle by improving code quality, documentation, and security integration. VMware-to-AWS migration strategies provide organizations with scalable, secure, and cost-effective modernization pathways. Security remains a top priority, with AWS emphasizing “security by design” and AI-driven detection and remediation. Lessons Learned from Vietnam Cloud Day 2025 Large-Scale Migration and Modernization with AWS Techcombank’s Challenge: Managing the complexity of migrating mission-critical banking systems while ensuring service continuity. Solution: Adopted a phased migration strategy, leveraging AWS migration accelerators and cloud-native monitoring to minimize risks. AWS Best Practice: Start with a comprehensive readiness assessment to identify risks early and design mitigation strategies. Balancing Modernization and Stability: Combined phased migration with modernization to maintain both speed and reliability. Modernizing Applications with Generative AI-Powered Tools Amazon Q Developer for Legacy Systems: Automates code analysis, refactoring, test generation, and documentation for Java/.NET workloads. OCB Experience: Integrated AI to achieve faster release cycles and improved maintainability in banking apps. DevSecOps Integration: Embedded security and compliance validation directly into the development pipeline. Application Modernization Panel Insights Financial Institutions vs. Startups: Banks focus on compliance and stability, while startups prioritize speed and agility. Common Ground: Both seek scalability and innovation via cloud-native modernization. Success Factor: Technology is key, but organizational culture and adoption mindset drive success. Ninety Eight’s Advantage: Early modernization with cloud-native/serverless gave agility and faster innovation compared to larger organizations. Transforming VMware with AI-Driven Modernization Reducing Downtime: Used automation playbooks and downtime-aware patterns for smooth VMware-to-AWS migration. Post-Migration Priority: Focused on cost optimization by right-sizing and shifting to managed services. Compliance: For financial firms, compliance requirements shaped the migration roadmap, ensuring audit-ready security. AWS Security at Scale AI for Threat Detection: Leveraged AI-driven anomaly detection and automated incident response. Shift-Left Security vs. Speed: Embedded security checks early in CI/CD to achieve both agility and compliance. Common Pitfalls: Weak IAM practices, underestimating shared responsibility, and delaying security integration. Case-Specific Experiences OCB (Orient Commercial Bank): Applied Amazon Q Developer within DevSecOps, accelerating development while ensuring regulatory compliance and data protection. LPBank Securities: Chose event-driven architecture (Kafka/Kinesis) to meet low-latency trading needs over microservices-only models. Ninety Eight (Startup): Adopted cloud-native and serverless-first to balance scalability and cost. Although initial investment was high, cost efficiency improved significantly after fine-tuning workloads. Some event photos Overall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives (per CloudJourney roadmap) Complete account setup and security configuration (Module 1). Master basic networking concepts: VPC, Subnet, Route Table, Security Group (Module 2). Set up the working environment (Hugo) and record logs in Markdown. Complete the hands-on labs on https://cloudjourney.awsstudygroup.com/ to verify work. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 1 — Account setup \u0026amp; security: Create AWS account, enable root MFA, create user groups \u0026amp; basic policies, configure billing alerts and budget. 08/09/2025 08/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 1 Tuesday Module 2 — Networking theory: Study VPC, Subnet, Route Table, Security Group, ENI/EIP, ELB concepts. Prepare Hugo environment. 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 2 Wednesday Module 2 — Labs: Create a VPC, public/private subnets, launch EC2 in a public subnet, configure Security Groups, create a NAT Gateway, SSH into EC2. 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 2 Labs Thursday Module 2 — Labs continued: Set up VPC Peering, configure NACLs, cross-VPC route tables; adjust templates (optimize instance types). 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 2 Labs Friday Module 3 — Storage overview \u0026amp; labs: EBS vs Instance Store, S3 static site hosting, S3 versioning \u0026amp; replication, Backup Plan \u0026amp; Restore; Storage Gateway lab. 12/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 3 Results \u0026amp; Achievements (Week 1) AWS account created and secured: root MFA enabled, admin user \u0026amp; group created, budget and billing alerts configured. Completed Module 2 theory and gained clear understanding of VPC, Subnet, Route Table, Security Group, and common connection patterns (NAT, Peering, Transit Gateway concepts). Hands-on labs performed: EC2 launched and accessed via SSH, Security Groups configured, NAT Gateway and VPC Peering created, NACLs applied, and template issues resolved (adjusted instance types). Storage labs completed: S3 static hosting with versioning and replication; Backup Plan/Vault configured; Storage Gateway set up. Hugo site initialized for reporting and the weekly worklog recorded in Markdown; project files organized. Issues Encountered \u0026amp; Mitigations Module 1 budget lab (Lab 7-3) showed an empty usage-type dropdown → logged the issue, retried later, will escalate to lab support if it persists. Some lab templates lacked Security Group rules → added minimal required rules (SSH/HTTP) to proceed. Uncleaned resources caused minor unexpected charges → performed resource cleanup and enabled stricter billing alerts. Next Steps (Week 2) Continue Module 3: EFS/FSx and MGN labs; deepen storage and backup practice. Start Module 4: Compute (ECS/EKS/Lambda) and integrate with the established networking setup. Prepare a short guide for the team on resource cleanup and cost optimization. Collect screenshots, logs, and scripts in the project folder for submission. "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Account Setup and Basic Networking (Modules 1 \u0026amp; 2)\nWeek 2: Advanced Storage and Compute Introduction (Modules 3 \u0026amp; 4)\nWeek 3: Advanced Compute and Databases (Module 4 \u0026amp; Databases)\nWeek 4: Expanded Databases and Monitoring (Databases \u0026amp; Monitoring)\nWeek 5: Server and Application Migration (Module 2)\nWeek 6: Database Migration and Hybrid Networking (Module 2)\nWeek 7: Cost Optimization and Performance (Module 3)\nWeek 8: Reliability and Advanced Security\nWeek 9: Microservices and Serverless\nWeek 10: Container Orchestration\nWeek 11: Data Lake and Analytics\nWeek 12: AI/ML Foundations\n"
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Student Information: Full Name: Dương Viết Huy\nPhone Number: 0858901719\nEmail: huydvse184528@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Assurance\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create CodePipeline and CodeBuild",
	"tags": [],
	"description": "",
	"content": "Create CodePipeline and CodeBuild In this section, you will create the AWS CodePipeline and CodeBuild resources to automate your CI/CD workflow.\nStep 1: Create IAM Roles First, create the necessary IAM roles for CodePipeline and CodeBuild.\n1.1 Create CodePipeline Role Navigate to IAM Console → Roles → Create Role\nSelect the following:\nTrusted entity type: AWS service Use case: CodePipeline Permissions: Attach the following policies: AWSCodePipelineFullAccess AmazonS3FullAccess AWSCodeBuildAdminAccess 1.2 Create CodeBuild Role Create another role for CodeBuild:\nTrusted entity type: AWS service Use case: CodeBuild Permissions: Attach the following policies: AWSCodeBuildAdminAccess AmazonEC2ContainerRegistryFullAccess AmazonS3FullAccess CloudWatchLogsFullAccess SecretsManagerReadWrite Step 2: Create S3 Bucket for Artifacts Create an S3 bucket to store pipeline artifacts:\naws s3 mb s3://secure-serverless-artifacts-$(aws sts get-caller-identity --query Account --output text) --region ap-southeast-1 Step 3: Create ECR Repository Create an ECR repository for container images:\naws ecr create-repository \\ --repository-name secure-serverless-app \\ --image-scanning-configuration scanOnPush=true \\ --region ap-southeast-1 Step 4: Create CodeBuild Project Navigate to CodeBuild Console → Create build project\nConfigure the following:\nSetting Value Project name secure-serverless-build Source provider GitLab Environment image Managed image Operating system Amazon Linux 2 Runtime Standard Image aws/codebuild/amazonlinux2-x86_64-standard:4.0 Privileged Enabled (for Docker builds) Service role CodeBuild role created earlier Buildspec Use a buildspec file buildspec.yml Create the buildspec file in your repository:\nversion: 0.2 env: variables: AWS_DEFAULT_REGION: ap-southeast-1 secrets-manager: GITLAB_TOKEN: \u0026#34;gitlab-credentials:token\u0026#34; phases: pre_build: commands: - echo Logging in to Amazon ECR... - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $ECR_REPO_URI - COMMIT_HASH=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7) - IMAGE_TAG=${COMMIT_HASH:=latest} build: commands: - echo Build started on `date` - echo Building the Docker image... - docker build -t $ECR_REPO_URI:latest . - docker tag $ECR_REPO_URI:latest $ECR_REPO_URI:$IMAGE_TAG post_build: commands: - echo Build completed on `date` - echo Pushing the Docker image... - docker push $ECR_REPO_URI:latest - docker push $ECR_REPO_URI:$IMAGE_TAG - printf \u0026#39;{\u0026#34;ImageURI\u0026#34;:\u0026#34;%s\u0026#34;}\u0026#39; $ECR_REPO_URI:$IMAGE_TAG \u0026gt; imageDetail.json artifacts: files: - imageDetail.json - appspec.yml - taskdef.json Step 5: Create CodePipeline Navigate to CodePipeline Console → Create pipeline\n5.1 Pipeline Settings Setting Value Pipeline name secure-serverless-pipeline Service role New service role or existing CodePipeline role Artifact store Custom location (S3 bucket created earlier) 5.2 Source Stage Setting Value Source provider GitLab Connection Create new connection or use existing Repository name your-gitlab-repo Branch name main 5.3 Build Stage Setting Value Build provider AWS CodeBuild Region Asia Pacific (Singapore) Project name secure-serverless-build 5.4 Deploy Stage Setting Value Deploy provider AWS Lambda Function name Your Lambda function name Input artifacts BuildArtifact Step 6: Verify Pipeline Push code to your GitLab repository Navigate to CodePipeline console Verify that the pipeline is triggered and runs successfully You can view detailed logs of each stage by clicking on the stage name and then \u0026ldquo;View logs\u0026rdquo; in CloudWatch.\nImages Required: codepipeline-role.png - IAM role creation for CodePipeline s3-artifact-bucket.png - S3 bucket for artifacts ecr-repository.png - ECR repository creation codebuild-project.png - CodeBuild project configuration source-stage.png - Pipeline source stage configuration pipeline-overview.png - Complete pipeline overview pipeline-execution.png - Successful pipeline execution "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Set up Lambda and API Gateway",
	"tags": [],
	"description": "",
	"content": "Set up Lambda and API Gateway In this section, you will create Lambda functions and configure API Gateway to expose them as REST APIs.\nStep 1: Create Lambda Execution Role Navigate to IAM Console → Roles → Create Role\nCreate a role with the following policies:\nAWSLambdaBasicExecutionRole AmazonDynamoDBFullAccess SecretsManagerReadWrite Step 2: Create Lambda Function 2.1 Using Terraform # lambda/main.tf resource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;api_handler\u0026#34; { function_name = \u0026#34;secure-serverless-api\u0026#34; role = aws_iam_role.lambda_exec.arn package_type = \u0026#34;Image\u0026#34; image_uri = \u0026#34;${var.ecr_repo_url}:latest\u0026#34; timeout = 30 memory_size = 256 environment { variables = { TABLE_NAME = var.dynamodb_table_name ENVIRONMENT = var.environment SECRETS_ARN = var.secrets_arn } } vpc_config { subnet_ids = var.subnet_ids security_group_ids = var.security_group_ids } tracing_config { mode = \u0026#34;Active\u0026#34; } } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;lambda_exec\u0026#34; { name = \u0026#34;lambda-exec-role\u0026#34; assume_role_policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [{ Action = \u0026#34;sts:AssumeRole\u0026#34; Effect = \u0026#34;Allow\u0026#34; Principal = { Service = \u0026#34;lambda.amazonaws.com\u0026#34; } }] }) } 2.2 Using AWS Console Navigate to Lambda Console → Create function Select Container image Configure: Function name: secure-serverless-api Container image URI: Your ECR image URI Execution role: Role created in Step 1 Step 3: Create API Gateway 3.1 Create REST API # api-gateway/main.tf resource \u0026#34;aws_api_gateway_rest_api\u0026#34; \u0026#34;main\u0026#34; { name = \u0026#34;secure-serverless-api\u0026#34; description = \u0026#34;Secure Serverless Application API\u0026#34; endpoint_configuration { types = [\u0026#34;REGIONAL\u0026#34;] } } resource \u0026#34;aws_api_gateway_resource\u0026#34; \u0026#34;items\u0026#34; { rest_api_id = aws_api_gateway_rest_api.main.id parent_id = aws_api_gateway_rest_api.main.root_resource_id path_part = \u0026#34;items\u0026#34; } resource \u0026#34;aws_api_gateway_method\u0026#34; \u0026#34;get_items\u0026#34; { rest_api_id = aws_api_gateway_rest_api.main.id resource_id = aws_api_gateway_resource.items.id http_method = \u0026#34;GET\u0026#34; authorization = \u0026#34;COGNITO_USER_POOLS\u0026#34; authorizer_id = aws_api_gateway_authorizer.cognito.id } resource \u0026#34;aws_api_gateway_integration\u0026#34; \u0026#34;lambda\u0026#34; { rest_api_id = aws_api_gateway_rest_api.main.id resource_id = aws_api_gateway_resource.items.id http_method = aws_api_gateway_method.get_items.http_method integration_http_method = \u0026#34;POST\u0026#34; type = \u0026#34;AWS_PROXY\u0026#34; uri = aws_lambda_function.api_handler.invoke_arn } 3.2 Configure CORS resource \u0026#34;aws_api_gateway_method\u0026#34; \u0026#34;options\u0026#34; { rest_api_id = aws_api_gateway_rest_api.main.id resource_id = aws_api_gateway_resource.items.id http_method = \u0026#34;OPTIONS\u0026#34; authorization = \u0026#34;NONE\u0026#34; } resource \u0026#34;aws_api_gateway_integration\u0026#34; \u0026#34;options\u0026#34; { rest_api_id = aws_api_gateway_rest_api.main.id resource_id = aws_api_gateway_resource.items.id http_method = aws_api_gateway_method.options.http_method type = \u0026#34;MOCK\u0026#34; request_templates = { \u0026#34;application/json\u0026#34; = \u0026#34;{\\\u0026#34;statusCode\\\u0026#34;: 200}\u0026#34; } } resource \u0026#34;aws_api_gateway_method_response\u0026#34; \u0026#34;options\u0026#34; { rest_api_id = aws_api_gateway_rest_api.main.id resource_id = aws_api_gateway_resource.items.id http_method = aws_api_gateway_method.options.http_method status_code = \u0026#34;200\u0026#34; response_parameters = { \u0026#34;method.response.header.Access-Control-Allow-Headers\u0026#34; = true \u0026#34;method.response.header.Access-Control-Allow-Methods\u0026#34; = true \u0026#34;method.response.header.Access-Control-Allow-Origin\u0026#34; = true } } Step 4: Deploy API resource \u0026#34;aws_api_gateway_deployment\u0026#34; \u0026#34;main\u0026#34; { rest_api_id = aws_api_gateway_rest_api.main.id depends_on = [ aws_api_gateway_integration.lambda ] lifecycle { create_before_destroy = true } } resource \u0026#34;aws_api_gateway_stage\u0026#34; \u0026#34;prod\u0026#34; { deployment_id = aws_api_gateway_deployment.main.id rest_api_id = aws_api_gateway_rest_api.main.id stage_name = \u0026#34;prod\u0026#34; xray_tracing_enabled = true access_log_settings { destination_arn = aws_cloudwatch_log_group.api_gw.arn format = jsonencode({ requestId = \u0026#34;$context.requestId\u0026#34; ip = \u0026#34;$context.identity.sourceIp\u0026#34; requestTime = \u0026#34;$context.requestTime\u0026#34; httpMethod = \u0026#34;$context.httpMethod\u0026#34; resourcePath = \u0026#34;$context.resourcePath\u0026#34; status = \u0026#34;$context.status\u0026#34; responseLength = \u0026#34;$context.responseLength\u0026#34; }) } } Step 5: Grant Lambda Permission resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;api_gw\u0026#34; { statement_id = \u0026#34;AllowExecutionFromAPIGateway\u0026#34; action = \u0026#34;lambda:InvokeFunction\u0026#34; function_name = aws_lambda_function.api_handler.function_name principal = \u0026#34;apigateway.amazonaws.com\u0026#34; source_arn = \u0026#34;${aws_api_gateway_rest_api.main.execution_arn}/*/*\u0026#34; } Step 6: Test the API # Get the API endpoint API_ENDPOINT=$(aws apigateway get-rest-apis --query \u0026#34;items[?name==\u0026#39;secure-serverless-api\u0026#39;].id\u0026#34; --output text) API_URL=\u0026#34;https://${API_ENDPOINT}.execute-api.ap-southeast-1.amazonaws.com/prod\u0026#34; # Test the endpoint (without auth - should fail) curl -X GET ${API_URL}/items # Test with Cognito token (will set up in next section) curl -X GET ${API_URL}/items \\ -H \u0026#34;Authorization: Bearer ${ID_TOKEN}\u0026#34; Images Required: lambda-role.png - IAM role for Lambda create-lambda.png - Lambda function creation api-gateway-console.png - API Gateway resources api-test.png - API testing with curl "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/5-workshop/5.1-workshop-overview/",
	"title": "Workshop Overview",
	"tags": [],
	"description": "",
	"content": "Introduction In this workshop, you will build a FastAPI Backend API running on AWS Lambda as a container image. This demonstrates modern cloud-native architecture combining serverless computing, container technology, and DevSecOps practices.\nWhat You Will Build By the end of this workshop, you will have:\nFastAPI Application\nRESTful API with 3 routers: auth, products, orders Layered architecture: API → Service → Repository Pydantic models for data validation JWT-based authentication Lambda Container Deployment\nFastAPI packaged as Docker container Mangum adapter for Lambda compatibility Container image stored in Amazon ECR Infrastructure as Code\nTerraform modules for all AWS resources DynamoDB tables with GSI indexes API Gateway HTTP API IAM roles with least privilege CI/CD Pipeline\nGitLab CI or AWS CodePipeline Semgrep for static code analysis Trivy for vulnerability scanning Automated deployment with Terraform Monitoring \u0026amp; Alerting\nCloudWatch log groups Lambda error alarms SNS notifications Architecture Flow ┌─────────────────────────────────────────────────────────────────┐ │ CI/CD Pipeline │ ├─────────────────────────────────────────────────────────────────┤ │ Git Push → Semgrep → Docker Build → Trivy → ECR → Terraform │ └─────────────────────────────────────────────────────────────────┘ ↓ ┌─────────────────────────────────────────────────────────────────┐ │ Runtime Architecture │ ├─────────────────────────────────────────────────────────────────┤ │ │ │ Client → API Gateway (HTTP) → Lambda (Container) │ │ ↓ │ │ ┌───────────────┐ │ │ │ Mangum │ │ │ └───────┬───────┘ │ │ ↓ │ │ ┌───────────────┐ │ │ │ FastAPI │ │ │ │ ├── /auth │ │ │ │ ├── /products │ │ │ └── /orders │ │ │ └───────┬───────┘ │ │ ↓ │ │ ┌─────────────────┼─────────────────┐ │ │ ↓ ↓ ↓ │ │ ┌──────────┐ ┌──────────┐ ┌──────────┐ │ │ │ Products │ │ Orders │ │ Users │ │ │ │ Table │ │ Table │ │ Table │ │ │ └──────────┘ └──────────┘ └──────────┘ │ │ DynamoDB │ └─────────────────────────────────────────────────────────────────┘ Key Technologies Component Technology Purpose Web Framework FastAPI High-performance async Python API Lambda Adapter Mangum ASGI adapter for AWS Lambda Container Docker Package application with dependencies Database DynamoDB NoSQL database with on-demand scaling Authentication JWT + Secrets Manager Secure token-based auth Infrastructure Terraform Infrastructure as Code Security Scanning Semgrep, Trivy SAST and vulnerability scanning CI/CD GitLab CI / CodePipeline Automated build and deploy FastAPI Application Structure # backend/app/main.py from fastapi import FastAPI from app.api.routers.auth import router as auth_router from app.api.routers.products import router as products_router from app.api.routers.orders import router as orders_router def create_app() -\u0026gt; FastAPI: app = FastAPI(title=\u0026#34;Products \u0026amp; Orders API\u0026#34;, version=\u0026#34;1.0.0\u0026#34;) @app.get(\u0026#34;/health\u0026#34;) async def health() -\u0026gt; dict: return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;} app.include_router(auth_router, prefix=\u0026#34;/auth\u0026#34;, tags=[\u0026#34;auth\u0026#34;]) app.include_router(products_router, prefix=\u0026#34;/products\u0026#34;, tags=[\u0026#34;products\u0026#34;]) app.include_router(orders_router, prefix=\u0026#34;/orders\u0026#34;, tags=[\u0026#34;orders\u0026#34;]) return app app = create_app() # backend/app/lambda_handler.py from mangum import Mangum from app.main import app handler = Mangum(app) # Lambda handler for API Gateway Terraform Module Structure # infra/main.tf module \u0026#34;dynamodb\u0026#34; { source = \u0026#34;./modules/dynamodb\u0026#34; products_table_name = var.products_table_name orders_table_name = var.orders_table_name users_table_name = var.users_table_name } module \u0026#34;iam\u0026#34; { source = \u0026#34;./modules/iam\u0026#34; project_name = var.project_name products_table_arn = module.dynamodb.products_table_arn orders_table_arn = module.dynamodb.orders_table_arn users_table_arn = module.dynamodb.users_table_arn jwt_secret_arn = var.jwt_secret_arn } module \u0026#34;lambda\u0026#34; { source = \u0026#34;./modules/lambda_container\u0026#34; project_name = var.project_name lambda_role_arn = module.iam.lambda_role_arn image_uri = var.image_uri environment = { APP_REGION = var.region PRODUCTS_TABLE = module.dynamodb.products_table_name ORDERS_TABLE = module.dynamodb.orders_table_name JWT_SECRET = data.aws_secretsmanager_secret_version.jwt.secret_string } } module \u0026#34;apigw\u0026#34; { source = \u0026#34;./modules/apigw\u0026#34; project_name = var.project_name lambda_arn = module.lambda.lambda_arn } module \u0026#34;observability\u0026#34; { source = \u0026#34;./modules/observability\u0026#34; project_name = var.project_name lambda_name = module.lambda.lambda_name } Learning Objectives After completing this workshop, you will be able to:\nBuild production-ready FastAPI applications Package Python apps as Lambda container images Design layered architecture (API → Service → Repository) Implement JWT authentication without Cognito Use Terraform modules for reusable infrastructure Integrate security scanning into CI/CD pipelines Set up monitoring and alerting for serverless applications Prerequisites Knowledge Basic Python programming Understanding of REST APIs Familiarity with Docker Basic AWS knowledge (Lambda, DynamoDB, IAM) Images Required: workshop-overview.png - Overall architecture diagram "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: “AI-Driven Development Workshop – Shaping the Future of Software Development” Event Overview Venue: AWS Event Hall, 26th Floor – Bitexco Tower, Ho Chi Minh City\nTime: 14:00 – 16:30, Friday, October 3, 2025\nSpeakers: Mr. Toan Huynh, Ms. My Nguyen\nCoordinators: Diem My, Dai Truong, Dinh Nguyen\nObjectives Introduce AI-Driven Development (AI-DD) and the AI-Driven Development Lifecycle (AI-DLC) framework. Demonstrate practical use of Amazon Q Developer and Kiro IDE Extension. Explore how AI improves speed, productivity, quality, and innovation in software development. Discuss the evolving relationship between AI and human developers in modern software projects. Workshop Highlights 1. Opening Session – “Shaping the Future of Development” Mr. Toan Huynh opened the workshop emphasizing the paradigm shift from traditional to AI-Orchestrated Development.\nIn this model, AI serves as a co-pilot—an orchestrator that assists in planning, design, programming, testing, and deployment, while developers maintain creative control and accountability.\n2. Challenges in Traditional Development Speakers pointed out limitations in current AI-assisted tools: lack of reliability, transparency, and contextual understanding.\nThe AI-Driven Development approach resolves these issues by ensuring that AI acts as an enabler, not a decision-maker—augmenting rather than replacing human expertise.\n3. The AI-Driven Development Lifecycle (AI-DLC) AI-DLC represents an evolution of development maturity across three stages:\nLevel Description AI-Assisted Development AI supports coding tasks such as code completion, syntax checking, and documentation. AI-Driven Development AI collaborates in system design, project planning, and decision support. AI-Managed Development AI orchestrates processes automatically with human oversight for validation. Within AI-DLC, AI functions as an intelligent coordinator, ensuring consistency while developers retain final decision-making authority.\n4. Benefits of AI Integration The session “AI in Development – Outcomes” highlighted seven major benefits:\nPredictability: Better project tracking and schedule accuracy. Velocity: Faster time-to-market for new features. Quality: Reduced bugs and improved stability. Innovation: Enabling creative and experimental approaches. Engagement: Enhancing developer motivation and efficiency. Customer Satisfaction: Improved user experiences. Productivity: Shorter development cycles and increased output. 5. AI in the SDLC Speakers analyzed how AI supports each SDLC phase:\nExplore \u0026amp; Plan → Create → Test \u0026amp; Secure → Review \u0026amp; Deploy → Maintain \u0026amp; Modernize.\nAI significantly shortens time in testing, deployment, and maintenance by automating repetitive workflows and analyzing logs intelligently.\n6. Standard AI-DLC Workflow A standard AI-DLC flow consists of four key steps:\nRequirement: Product Owner collects and analyzes user needs. Design: Architects define structure, APIs, and processes. Implementation: Engineers develop and integrate code. Deployment: System is launched and monitored. AI tools assist in every step—ensuring output consistency and better communication between roles.\n7. Key Process Features The “Key Workflow Features” slide described four main characteristics:\nRole Separation: Clear distinction between business, design, and implementation tasks. AI-Enhanced Contexts: Each role uses AI differently (PM, Architect, Developer). Iterative Feedback Loops: Continuous improvement between stages. Template-Driven Approach: Standardized documentation for maintainability. 8. AI in Practice – Live Demonstrations Amazon Q Developer Integrated into IDEs such as VS Code and AWS Cloud9. Generates, tests, and documents code automatically. Assists in updating prompt.md, managing CI/CD pipelines, and suggesting AWS architectures. Demo showcased how AI creates project plans, user stories, and AWS-based architecture diagrams. Kiro IDE (Presented by Ms. My Nguyen) A powerful extension for creating structured documents: requirements.md, design.md, tasks.md. AI features include generating feature specs, defining API flows, and backend code creation. Demonstration: Building a Chat Application with authentication, including login, registration, and token handling. Key Insights and Takeaways AI should be viewed as a smart collaborator, augmenting human work across all development phases. The AI-DLC model standardizes workflow and enhances cross-team transparency. Amazon Q Developer improves productivity by automating documentation and testing. Kiro IDE demonstrates how AI can accelerate backend generation and design tasks. Integrating AI into DevSecOps is becoming a necessity for productivity and security alignment. Future developers will act as AI orchestrators—guiding, validating, and ethically deploying AI-driven processes. Practical Applications Deploy Amazon Q Developer to automate testing, code documentation, and CI/CD integration. Utilize Kiro IDE to standardize specifications and reduce manual workload. Conduct AI-Driven Sprints to measure productivity gains. Implement AI-Assisted Code Review to enhance software quality and maintain compliance. “The workshop reaffirmed that AI is not a replacement but a catalyst for innovation—enabling developers to build smarter, faster, and more reliable systems.”\nSome event photos Personal Reflection:\nThe event provided deep and practical insight into how AI redefines software engineering.\nMr. Toan Huynh’s strategic perspective and Ms. My Nguyen’s technical demonstration together highlighted that the future of software lies in collaboration between human creativity and AI precision.\n"
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives (per CloudJourney roadmap) Deepen storage knowledge with EFS, FSx, and migration tools (Module 3). Introduce compute services: EC2 advanced features, Auto Scaling Groups, and initial container concepts (Module 4). Integrate compute with existing VPC and storage setups. Update Hugo site with Week 2 logs and organize lab artifacts. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 3 — Storage advanced: Study EFS shared file systems, FSx for Windows/Linux, configure EFS for EC2 access. 15/09/2025 15/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 3 Tuesday Module 3 — Labs: Set up EFS mount targets, FSx file shares, perform data migration using AWS MGN for storage workloads. 16/09/2025 16/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 3 Labs Wednesday Module 4 — Compute intro: Review EC2 instance types, launch templates, create Auto Scaling Groups integrated with VPC. 17/09/2025 17/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 4 Thursday Module 4 — Labs: Deploy EC2 ASG with load balancer, simulate scaling events, attach EBS volumes dynamically. 18/09/2025 18/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 4 Labs Friday Backup and restore practice: Test EFS backups with AWS Backup, review cost implications of storage/compute setups. 19/09/2025 19/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 3/4 Results \u0026amp; Achievements (Week 2) Advanced storage configured: EFS mounted on multiple EC2 instances for shared access, FSx set up for managed file systems, MGN used for initial storage migration simulation. Compute foundation built: ASG deployed with ELB for high availability, scaling policies tested under load, integrated with existing VPC and EBS/EFS. Backup strategies implemented: Automated backups for EFS and EC2 volumes via AWS Backup, restore procedures verified. Resource organization improved: Created a team guide on cleanup and cost optimization, shared via Hugo site. Week 2 logs documented, screenshots of ASG scaling and EFS mounts added to repository. Issues Encountered \u0026amp; Mitigations EFS lab encountered mount permission errors → adjusted security groups and IAM roles for NFS access. ASG scaling failed initially due to IAM policy gaps → added necessary EC2 and AutoScaling permissions. Minor cost spikes from untagged resources → implemented tagging policies and reviewed with Cost Explorer. Next Steps (Week 3) Advance Module 4: Dive into ECS, EKS, and Lambda for containerized and serverless compute. Begin databases: RDS setup and integration with compute services. Experiment with basic monitoring using CloudWatch. Prepare integration demo of storage-compute-database stack. "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Configure DynamoDB",
	"tags": [],
	"description": "",
	"content": "Configure DynamoDB In this section, you will create and configure a DynamoDB table for storing application data.\nStep 1: Design Table Schema For our serverless application, we\u0026rsquo;ll create a table with the following design:\nAttribute Type Key Type PK String Partition Key SK String Sort Key data Map - createdAt String - updatedAt String - Step 2: Create DynamoDB Table with Terraform # dynamodb/main.tf resource \u0026#34;aws_dynamodb_table\u0026#34; \u0026#34;main\u0026#34; { name = \u0026#34;secure-serverless-table\u0026#34; billing_mode = \u0026#34;PAY_PER_REQUEST\u0026#34; # On-demand capacity hash_key = \u0026#34;PK\u0026#34; range_key = \u0026#34;SK\u0026#34; attribute { name = \u0026#34;PK\u0026#34; type = \u0026#34;S\u0026#34; } attribute { name = \u0026#34;SK\u0026#34; type = \u0026#34;S\u0026#34; } attribute { name = \u0026#34;GSI1PK\u0026#34; type = \u0026#34;S\u0026#34; } attribute { name = \u0026#34;GSI1SK\u0026#34; type = \u0026#34;S\u0026#34; } # Global Secondary Index global_secondary_index { name = \u0026#34;GSI1\u0026#34; hash_key = \u0026#34;GSI1PK\u0026#34; range_key = \u0026#34;GSI1SK\u0026#34; projection_type = \u0026#34;ALL\u0026#34; } # Enable Point-in-Time Recovery point_in_time_recovery { enabled = true } # Enable server-side encryption with KMS server_side_encryption { enabled = true kms_key_arn = var.kms_key_arn } # Enable TTL for automatic item expiration ttl { attribute_name = \u0026#34;expiresAt\u0026#34; enabled = true } tags = { Environment = var.environment Project = \u0026#34;secure-serverless\u0026#34; } } Step 3: Create via AWS Console Navigate to DynamoDB Console → Create table Configure: Table name: secure-serverless-table Partition key: PK (String) Sort key: SK (String) Table settings: Customize settings Configure capacity:\nSelect On-demand for automatic scaling Enable encryption:\nSelect AWS managed key or your custom KMS key Step 4: Create Global Secondary Index # Add GSI for querying by different access patterns global_secondary_index { name = \u0026#34;GSI1\u0026#34; hash_key = \u0026#34;GSI1PK\u0026#34; range_key = \u0026#34;GSI1SK\u0026#34; projection_type = \u0026#34;ALL\u0026#34; } Step 5: Configure Auto Scaling (Optional) If using provisioned capacity:\nresource \u0026#34;aws_appautoscaling_target\u0026#34; \u0026#34;read_target\u0026#34; { max_capacity = 100 min_capacity = 5 resource_id = \u0026#34;table/${aws_dynamodb_table.main.name}\u0026#34; scalable_dimension = \u0026#34;dynamodb:table:ReadCapacityUnits\u0026#34; service_namespace = \u0026#34;dynamodb\u0026#34; } resource \u0026#34;aws_appautoscaling_policy\u0026#34; \u0026#34;read_policy\u0026#34; { name = \u0026#34;DynamoDBReadAutoScaling\u0026#34; policy_type = \u0026#34;TargetTrackingScaling\u0026#34; resource_id = aws_appautoscaling_target.read_target.resource_id scalable_dimension = aws_appautoscaling_target.read_target.scalable_dimension service_namespace = aws_appautoscaling_target.read_target.service_namespace target_tracking_scaling_policy_configuration { predefined_metric_specification { predefined_metric_type = \u0026#34;DynamoDBReadCapacityUtilization\u0026#34; } target_value = 70 } } Step 6: Lambda Integration Update your Lambda function to interact with DynamoDB:\nimport boto3 import os from datetime import datetime dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(os.environ[\u0026#39;TABLE_NAME\u0026#39;]) def create_item(user_id, item_data): \u0026#34;\u0026#34;\u0026#34;Create a new item in DynamoDB\u0026#34;\u0026#34;\u0026#34; timestamp = datetime.utcnow().isoformat() item = { \u0026#39;PK\u0026#39;: f\u0026#39;USER#{user_id}\u0026#39;, \u0026#39;SK\u0026#39;: f\u0026#39;ITEM#{timestamp}\u0026#39;, \u0026#39;GSI1PK\u0026#39;: \u0026#39;ITEM\u0026#39;, \u0026#39;GSI1SK\u0026#39;: timestamp, \u0026#39;data\u0026#39;: item_data, \u0026#39;createdAt\u0026#39;: timestamp, \u0026#39;updatedAt\u0026#39;: timestamp } table.put_item(Item=item) return item def get_user_items(user_id): \u0026#34;\u0026#34;\u0026#34;Get all items for a user\u0026#34;\u0026#34;\u0026#34; response = table.query( KeyConditionExpression=\u0026#39;PK = :pk AND begins_with(SK, :sk)\u0026#39;, ExpressionAttributeValues={ \u0026#39;:pk\u0026#39;: f\u0026#39;USER#{user_id}\u0026#39;, \u0026#39;:sk\u0026#39;: \u0026#39;ITEM#\u0026#39; } ) return response[\u0026#39;Items\u0026#39;] def get_all_items(): \u0026#34;\u0026#34;\u0026#34;Get all items using GSI\u0026#34;\u0026#34;\u0026#34; response = table.query( IndexName=\u0026#39;GSI1\u0026#39;, KeyConditionExpression=\u0026#39;GSI1PK = :pk\u0026#39;, ExpressionAttributeValues={ \u0026#39;:pk\u0026#39;: \u0026#39;ITEM\u0026#39; }, ScanIndexForward=False # Newest first ) return response[\u0026#39;Items\u0026#39;] Step 7: Verify Table Creation # List tables aws dynamodb list-tables # Describe table aws dynamodb describe-table --table-name secure-serverless-table # Test put item aws dynamodb put-item \\ --table-name secure-serverless-table \\ --item \u0026#39;{\u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;USER#test\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;ITEM#001\u0026#34;}, \u0026#34;data\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;test data\u0026#34;}}\u0026#39; # Query items aws dynamodb query \\ --table-name secure-serverless-table \\ --key-condition-expression \u0026#34;PK = :pk\u0026#34; \\ --expression-attribute-values \u0026#39;{\u0026#34;:pk\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;USER#test\u0026#34;}}\u0026#39; Use single-table design patterns in DynamoDB for better performance and cost efficiency. Design your access patterns first, then create the table schema.\nImages Required: dynamodb-design.png - Table schema design diagram create-table.png - DynamoDB create table console gsi-config.png - Global Secondary Index configuration dynamodb-items.png - Items in DynamoDB table "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Configure Security Scanning",
	"tags": [],
	"description": "",
	"content": "Configure Security Scanning with Semgrep and Trivy In this section, you will integrate security scanning tools into your CI/CD pipeline to automatically detect vulnerabilities.\nOverview Tool Purpose Scan Type Semgrep Static Application Security Testing Source code Trivy Container Vulnerability Scanner Container images Step 1: Configure Semgrep Semgrep is a fast, open-source static analysis tool that finds bugs and enforces code standards.\n1.1 Create Semgrep Configuration Create .semgrep.yml in your repository root:\nrules: - id: hardcoded-aws-credentials patterns: - pattern-either: - pattern: AKIA... - pattern: aws_access_key_id = \u0026#34;...\u0026#34; - pattern: aws_secret_access_key = \u0026#34;...\u0026#34; message: \u0026#34;Hardcoded AWS credentials detected\u0026#34; languages: [python, javascript, typescript, yaml] severity: ERROR - id: sql-injection patterns: - pattern: | $QUERY = \u0026#34;...\u0026#34; + $USER_INPUT + \u0026#34;...\u0026#34; $DB.execute($QUERY) message: \u0026#34;Potential SQL injection vulnerability\u0026#34; languages: [python] severity: ERROR - id: insecure-lambda-permissions patterns: - pattern: | Effect: Allow Action: \u0026#34;*\u0026#34; Resource: \u0026#34;*\u0026#34; message: \u0026#34;Overly permissive Lambda IAM policy\u0026#34; languages: [yaml] severity: WARNING 1.2 Add Semgrep to CodeBuild Update your buildspec.yml to include Semgrep scanning:\nversion: 0.2 phases: install: commands: - pip install semgrep pre_build: commands: - echo \u0026#34;Running Semgrep security scan...\u0026#34; - semgrep --config=auto --config=.semgrep.yml --error --json --output=semgrep-results.json . || true - | if [ -s semgrep-results.json ]; then echo \u0026#34;Security vulnerabilities found!\u0026#34; cat semgrep-results.json SEVERITY=$(jq \u0026#39;.results[].extra.severity\u0026#39; semgrep-results.json | grep -c \u0026#34;ERROR\u0026#34; || true) if [ \u0026#34;$SEVERITY\u0026#34; -gt \u0026#34;0\u0026#34; ]; then echo \u0026#34;Critical vulnerabilities detected. Failing build.\u0026#34; exit 1 fi fi - echo \u0026#34;Semgrep scan completed\u0026#34; Step 2: Configure Trivy Trivy is a comprehensive security scanner for containers, detecting vulnerabilities in OS packages and application dependencies.\n2.1 Add Trivy to CodeBuild Update your buildspec.yml to include Trivy scanning:\nversion: 0.2 phases: install: commands: - pip install semgrep - curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin pre_build: commands: # Semgrep scan - echo \u0026#34;Running Semgrep security scan...\u0026#34; - semgrep --config=auto --error . || exit 1 # ECR login - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $ECR_REPO_URI build: commands: - echo \u0026#34;Building Docker image...\u0026#34; - docker build -t $ECR_REPO_URI:$CODEBUILD_RESOLVED_SOURCE_VERSION . post_build: commands: # Trivy scan - echo \u0026#34;Running Trivy container scan...\u0026#34; - trivy image --exit-code 0 --severity LOW,MEDIUM --format json --output trivy-low-medium.json $ECR_REPO_URI:$CODEBUILD_RESOLVED_SOURCE_VERSION - trivy image --exit-code 1 --severity HIGH,CRITICAL --format json --output trivy-high-critical.json $ECR_REPO_URI:$CODEBUILD_RESOLVED_SOURCE_VERSION # Push image if scan passes - docker push $ECR_REPO_URI:$CODEBUILD_RESOLVED_SOURCE_VERSION artifacts: files: - semgrep-results.json - trivy-low-medium.json - trivy-high-critical.json Step 3: Create Security Reports Dashboard 3.1 Store Reports in S3 Add the following to your buildspec to store security reports:\nartifacts: files: - semgrep-results.json - trivy-*.json name: security-reports-$CODEBUILD_BUILD_NUMBER reports: security-report: files: - semgrep-results.json - trivy-high-critical.json file-format: GENERICJSON 3.2 Set Up CloudWatch Metrics Create CloudWatch metrics for security findings:\n# Create custom metric for security findings aws cloudwatch put-metric-data \\ --namespace \u0026#34;SecurityScanning\u0026#34; \\ --metric-name \u0026#34;HighSeverityFindings\u0026#34; \\ --value 0 \\ --dimensions Pipeline=secure-serverless-pipeline Step 4: Configure Failure Thresholds Create a security policy that fails builds based on severity:\nSeverity Action CRITICAL Fail build immediately HIGH Fail build immediately MEDIUM Warning, allow build to continue LOW Informational only Example Policy Implementation # security-policy.yml thresholds: critical: 0 # Zero tolerance for critical high: 0 # Zero tolerance for high medium: 10 # Allow up to 10 medium low: 50 # Allow up to 50 low actions: on_critical: block on_high: block on_medium: warn on_low: info Step 5: Verify Security Scanning Push code with a deliberate vulnerability (for testing): # test_vulnerability.py (DO NOT USE IN PRODUCTION) import os password = \u0026#34;hardcoded_password_123\u0026#34; # This should trigger Semgrep Check the CodeBuild logs for security findings Verify that the build fails due to security violations Best Practices Security Scanning Best Practices:\nRun security scans early in the pipeline (shift-left security) Set appropriate severity thresholds based on your risk tolerance Regularly update Semgrep rules and Trivy database Review and address all findings, even low severity ones Store security reports for compliance and auditing Images Required: security-scanning-flow.png - Security scanning workflow diagram semgrep-results.png - Example Semgrep scan results trivy-results.png - Example Trivy scan results security-scan-failure.png - Build failure due to security issues security-dashboard.png - CloudWatch security metrics dashboard "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/5-workshop/5.2-prerequiste/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Prerequisites Before starting this workshop, ensure you have the following requirements ready.\n1. AWS Account You need an AWS account with appropriate permissions. If you don\u0026rsquo;t have one:\nGo to AWS Console Click Create an AWS Account Follow the registration process Enable MFA for the root account (recommended) This workshop will create resources that incur costs. Estimated cost is ~$5-10 if cleaned up within a few hours. Make sure to complete the cleanup section at the end.\n2. IAM User with Required Permissions Create an IAM user with the following managed policies:\nAdministratorAccess (for workshop purposes only) Or create a custom policy with permissions for:\nLambda, API Gateway, DynamoDB CodePipeline, CodeBuild, CodeDeploy CloudFront, Route 53, WAF Cognito, Secrets Manager, KMS CloudWatch, CloudTrail, GuardDuty, EventBridge, SNS IAM, ECR, S3 3. GitLab Account Sign up at GitLab Create a new project/repository Generate a Personal Access Token with api and write_repository scopes 4. Local Development Environment Install the following tools on your local machine:\nTool Version Purpose AWS CLI v2.x Interact with AWS services Terraform v1.5+ Infrastructure as Code Git Latest Version control Docker Latest Container building Node.js v18+ Lambda function development Python v3.9+ Lambda function development Install AWS CLI # Windows (PowerShell) msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi # macOS brew install awscli # Linux curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Configure AWS CLI aws configure # Enter your Access Key ID # Enter your Secret Access Key # Enter default region: ap-southeast-1 # Enter output format: json Install Terraform # Windows (Chocolatey) choco install terraform # macOS brew tap hashicorp/tap brew install hashicorp/tap/terraform # Linux sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y gnupg software-properties-common wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg echo \u0026#34;deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026#34; | sudo tee /etc/apt/sources.list.d/hashicorp.list sudo apt update \u0026amp;\u0026amp; sudo apt install terraform 5. Domain Name (Optional) For Route 53 and CloudFront configuration, you can optionally use:\nA registered domain name, OR Use CloudFront\u0026rsquo;s default domain for testing 6. Verify Setup Run the following commands to verify your setup:\n# Check AWS CLI aws --version aws sts get-caller-identity # Check Terraform terraform --version # Check Git git --version # Check Docker docker --version # Check Node.js node --version npm --version 7. Clone Workshop Repository git clone https://gitlab.com/your-username/secure-serverless-workshop.git cd secure-serverless-workshop Workshop Resources Download Download the starter code and Terraform modules:\nThe workshop repository contains:\nTerraform modules for all AWS resources Sample Lambda function code GitLab CI/CD configuration files Security scanning configuration Images Required for this section: aws-account.png - AWS account creation/login screen gitlab-setup.png - GitLab project and token creation verify-setup.png - Terminal showing successful version checks iam-permissions.png - IAM user permissions configuration "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Secure Serverless Application Proposal 1. Project Summary The project operates a serverless application on AWS with a fully automated CI/CD pipeline. When code is pushed to GitLab, the pipeline automatically scans for security vulnerabilities (Semgrep, Trivy), builds containers, provisions infrastructure via Terraform, and deploys images to Amazon ECR. Users access the application through Route 53 → CloudFront → WAF → API Gateway, authenticated by Cognito, invoking Lambda functions for business logic and storing data in DynamoDB. The system continuously monitors through CloudWatch, CloudTrail, and GuardDuty, sending alerts via EventBridge → SNS upon incidents or threats.\n2. Problem Statement Current Problems Many organizations struggle with deploying serverless applications due to error-prone manual processes, lack of security controls, and scalability challenges. Specific issues include:\nManual deployment processes lack consistent security checks, often missing vulnerability scanning and automated testing steps Existing systems do not provide high-performance content delivery (CDN) nor application-layer protection against DDoS and bot attacks Secrets, encryption keys, and IAM permissions are difficult to manage end-to-end across the CI/CD pipeline while maintaining least-privilege principles Limited observability causes slow incident response and delayed threat detection Solution The project delivers a complete serverless architecture with automated CI/CD, integrated security, and comprehensive monitoring. The deployment pipeline automates from GitLab through CodePipeline, CodeBuild, Semgrep (SAST), Trivy (container scanning), Terraform (IaC), and CodeDeploy, ensuring every change is tested and security-scanned. The delivery layer uses Route 53, CloudFront, and WAF to accelerate and protect the application. The serverless backend with Cognito (authentication), API Gateway, Lambda, DynamoDB, Secrets Manager, and KMS ensures data security. A multi-layer monitoring system (CloudWatch, CloudTrail, GuardDuty, EventBridge, SNS) detects and alerts on incidents instantly.\nBenefits and Return on Investment (ROI) The solution delivers significant technical and financial benefits: full automation reduces deployment time by 80-90% and eliminates manual errors, integrated security reduces vulnerability risks, CDN and serverless architecture ensure high performance with automatic scaling, and centralized monitoring enables 70-80% faster incident detection. Financially, operational costs are estimated at ~$20-40/month for small to medium scale, significantly lower than traditional infrastructure, with no upfront hardware investment and 60-70% reduction in operational labor costs. Expected payback period is 1-2 months, while providing an extensible foundation for future projects.\n3. System Architecture The architecture is divided into three domains:\nCI/CD Pipeline: Running on GitLab and AWS CodePipeline to build containers, scan security (Semgrep, Trivy), provision infrastructure via Terraform, and deploy images to Amazon ECR Content Delivery \u0026amp; Protection Layer: Using Route 53, AWS WAF, and CloudFront to accelerate and secure user access Serverless Application Core: Located in ap-southeast-1 region with Cognito (authentication), API Gateway, Lambda (business logic), DynamoDB (storage), KMS and Secrets Manager (security), and the observability suite (CloudWatch, CloudTrail, GuardDuty, EventBridge, SNS) AWS Services Used\nGitLab Actions, AWS CodePipeline, CodeBuild, CodeDeploy Semgrep, Trivy Terraform, Amazon ECR Amazon Cognito, Amazon API Gateway, AWS Lambda, Amazon DynamoDB AWS WAF, Amazon CloudFront, Amazon Route 53 AWS Secrets Manager, AWS Key Management Service (KMS) Amazon CloudWatch, AWS CloudTrail, Amazon GuardDuty, Amazon EventBridge, Amazon SNS Component Design\nCI/CD: A Git push triggers the pipeline to run Semgrep (SAST), build containers in CodeBuild, scan with Trivy, execute Terraform Plan/Apply, and deliver via CodeDeploy. Delivery \u0026amp; Protection: Route 53 handles DNS → CloudFront caches content → WAF filters malicious traffic before reaching the APIs. Application Services: Cognito issues tokens, API Gateway validates requests and forwards them to Lambda, which processes business logic and reads/writes DynamoDB. Secrets \u0026amp; Encryption: Secrets Manager stores sensitive information, KMS encrypts data and keys. Monitoring \u0026amp; Alerting: CloudWatch aggregates metrics/logs, CloudTrail records audit trails, GuardDuty detects threats, EventBridge routes anomalies to SNS notifications. 4. Technical Implementation Model the infrastructure with Terraform: logical VPC boundaries, least-privilege IAM roles, API Gateway, Lambda, DynamoDB, and security controls. Configure GitLab CI/CD with AWS CodePipeline/CodeBuild/CodeDeploy using cross-account IAM roles. Enforce Semgrep and Trivy scans in every pipeline run, failing builds on high-severity findings. Define Terraform modules for Lambda (container images in ECR), API Gateway, CloudFront, WAF, and Cognito resources. Enable CloudWatch log groups, metric filters, GuardDuty, CloudTrail; create EventBridge rules that fan out to SNS alerts. Implement automated rollback via CodeDeploy deployment groups and versioned Terraform state. 5. Timeline \u0026amp; Milestones Phase Schedule Key Deliverables Kick-off Week 1 Requirements intake, detailed design, IAM role matrix IaC \u0026amp; CI/CD Setup Week 2 Terraform baseline, GitLab pipeline, CodePipeline integration Security Integration Week 3 Semgrep, Trivy, WAF rules, Cognito, Secrets Manager Backend Completion Week 4 Lambda handlers, API Gateway routes, DynamoDB schema, unit tests Deployment \u0026amp; Testing Week 5 End-to-end pipeline run, integration and CDN performance testing Operations \u0026amp; Handover Week 6 Optimize monitoring systems, configure intelligent alerting, finalize documentation Post-Deployment Week 7+ Continuous performance and cost monitoring, configuration optimization, feature expansion as needed, periodic security updates 6. Budget Estimation AWS Service Primary Billing Factor Estimated Cost (USD) AWS Lambda \u0026amp; API Gateway 250,000 requests/month ~$1.00 - $3.00 Amazon DynamoDB 5 GB storage, 5 RCU/5 WCU (Provisioned) ~$5.00 - $10.00 CI/CD (CodePipeline/CodeBuild) 5 deployments (250 build minutes) ~$2.25 - $4.00 Amazon ECR 5 GB image storage ($0.10/GB) ~$0.50 - $1.50 Route 53 1 Hosted Zone + queries ~$0.54 CloudFront / WAF 15 GB Data Transfer Out, basic WAF usage ~$9.50 - $15.50 Security \u0026amp; Monitoring GuardDuty, KMS, Secrets Manager, minimal logs ~$2.00 - $5.00 Total Cost ~$20.79 - $39.54 USD/month Note: Actual spend varies with traffic and configuration; costs can drop further by shutting down dev/test environments. No hardware expenditure required.\n7. Risk Assessment Risk Impact Likelihood Mitigation Undetected vulnerabilities High Medium Mandatory Semgrep/Trivy scans, enable AWS Inspector, periodic manual reviews Misconfigured Terraform causing downtime High Low Peer review Terraform plans, use staging environments, back up state Unexpected CDN or Terraform cost spikes Medium Medium Configure AWS Budgets, tune CloudFront cache policies, monitor spend Cognito authentication issues Medium Low Implement automated auth tests, maintain backup user pool in another Region Mitigation Strategies Security: Integrate automated security scanning (Semgrep, Trivy) into every pipeline, enable AWS Inspector periodically, conduct code reviews and quarterly security audits Infrastructure Configuration: Apply Infrastructure as Code (Terraform) with version control, peer review for all changes, use staging environments for testing before production, regularly backup Terraform state Cost Management: Set up AWS Budgets with 80% and 100% alert thresholds, configure optimized CloudFront cache policies, monitor costs daily via Cost Explorer, automatically shut down dev/test resources when unused Authentication \u0026amp; Access: Deploy automated testing for API authentication, maintain backup Cognito user pool in another region, enable multi-factor authentication (MFA) for administrative accounts Contingency Plan Security Incidents: Upon detection of critical vulnerabilities, immediately rollback to previous version via CodeDeploy, isolate affected resources, notify security team and proceed with patching Downtime from Misconfiguration: Use Terraform state backup to restore previous configuration, temporarily route traffic to staging environment if needed, maintain runbook for quick rollback Budget Overrun: Automatically shut down non-essential services when alert threshold is reached, optimize CloudFront cache hit ratio, consider switching to Reserved Capacity for DynamoDB if long-term usage Authentication Failures: Switch to backup Cognito user pool in another region, utilize API Gateway caching to reduce load, maintain fallback authentication mechanism 8. Expected Outcomes A secure, fully automated CI/CD pipeline where every change is scanned and tested before reaching production. A low-latency serverless backend served through CDN and API Gateway with pervasive data encryption. Centralized observability that surfaces incidents instantly and supports high availability SLAs. Optimized access control and cost efficiency delivered through least-privilege IAM and serverless scaling. An extensible infrastructure foundation ready for future features or additional AWS Regions. "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "AI/ML/GenAI on AWS Workshop Event Information Date \u0026amp; Time: Saturday, November 15, 2025, 8:30 AM – 12:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent Purpose The workshop was designed to provide hands-on experience with AWS AI/ML services, focusing on Amazon SageMaker for traditional machine learning workflows and Amazon Bedrock for generative AI applications. The event aimed to help participants understand the practical implementation of AI/ML solutions on AWS and explore the latest capabilities in generative AI.\nAgenda Overview 8:30 – 9:00 AM | Welcome \u0026amp; Introduction Participant registration and networking opportunities Workshop overview and learning objectives presentation Ice-breaker activity to foster collaboration Overview of the AI/ML landscape in Vietnam 9:00 – 10:30 AM | AWS AI/ML Services Overview Amazon SageMaker – End-to-End ML Platform\nData Preparation and Labeling: Understanding how to prepare datasets for machine learning, including data cleaning, feature engineering, and automated labeling capabilities Model Training, Tuning, and Deployment: Exploring SageMaker\u0026rsquo;s training infrastructure, hyperparameter tuning, and model deployment options including real-time and batch inference Integrated MLOps Capabilities: Learning about SageMaker\u0026rsquo;s built-in MLOps features for model versioning, monitoring, and automated retraining pipelines Live Demo: SageMaker Studio Walkthrough\nThe demonstration showcased the unified development environment for machine learning, including:\nJupyter notebook integration for interactive development Experiment tracking and model registry Visual workflow builder for MLOps pipelines Integration with other AWS services for data processing 10:30 – 10:45 AM | Coffee Break Networking session with refreshments and informal discussions about AI/ML use cases.\n10:45 AM – 12:00 PM | Generative AI with Amazon Bedrock Foundation Models: Claude, Llama, Titan – Comparison \u0026amp; Selection Guide\nUnderstanding different foundation models available on Bedrock Comparison of model capabilities, use cases, and performance characteristics Best practices for selecting the right model for specific business needs Cost considerations and optimization strategies Prompt Engineering: Techniques, Chain-of-Thought Reasoning, Few-shot Learning\nPrompt Engineering Fundamentals: Learning how to craft effective prompts to get desired outputs from language models Chain-of-Thought Reasoning: Understanding how to guide models through step-by-step reasoning processes for complex problem-solving Few-shot Learning: Techniques for providing examples to improve model performance on specific tasks without fine-tuning Retrieval-Augmented Generation (RAG): Architecture \u0026amp; Knowledge Base Integration\nRAG Architecture Overview: Understanding how RAG combines retrieval of relevant information with generative capabilities Knowledge Base Integration: Learning to connect Bedrock with vector databases and knowledge bases (Amazon OpenSearch, Amazon Kendra) Implementation Patterns: Best practices for building RAG applications that provide accurate, context-aware responses Bedrock Agents: Multi-step Workflows and Tool Integrations\nAgent Architecture: Understanding how Bedrock Agents can orchestrate complex multi-step workflows Tool Integration: Learning to connect agents with external APIs, databases, and AWS services Workflow Design: Patterns for designing agent-based applications that can handle complex user requests Guardrails: Safety and Content Filtering\nContent Safety: Understanding Bedrock Guardrails for filtering harmful or inappropriate content Custom Policies: Learning to configure custom content filters based on business requirements Compliance and Governance: Best practices for ensuring AI applications meet regulatory and ethical standards Live Demo: Building a Generative AI Chatbot using Bedrock\nThe demonstration walked through creating a complete chatbot application:\nSetting up Bedrock foundation model Implementing RAG with knowledge base integration Configuring Bedrock Agents for multi-turn conversations Adding Guardrails for content safety Deploying the chatbot application Key Highlights Comprehensive ML Platform: SageMaker provides a complete end-to-end solution for machine learning, from data preparation to model deployment and monitoring Generative AI Capabilities: Amazon Bedrock offers access to multiple foundation models, making it easy to experiment and choose the right model for each use case RAG Architecture: The RAG pattern enables building AI applications that can access and utilize specific knowledge bases, improving accuracy and relevance Production-Ready MLOps: SageMaker\u0026rsquo;s integrated MLOps capabilities simplify the process of deploying and maintaining ML models in production Safety First: Bedrock Guardrails ensure that generative AI applications are safe, compliant, and aligned with business values Key Learnings SageMaker Studio provides a unified interface for the entire ML lifecycle, significantly improving developer productivity Foundation model selection is crucial and depends on specific use cases, performance requirements, and cost constraints Prompt engineering is a critical skill that can dramatically improve model outputs without requiring fine-tuning RAG architecture is essential for building AI applications that need access to specific, up-to-date information Bedrock Agents enable building sophisticated AI applications that can handle complex, multi-step workflows Content safety must be considered from the beginning when building generative AI applications Application to My Work Experiment with SageMaker: Set up SageMaker Studio to explore ML model development for data analysis projects Build RAG Applications: Implement RAG architecture using Bedrock and knowledge bases for internal documentation and Q\u0026amp;A systems Prompt Engineering Practice: Develop prompt engineering skills by creating templates and best practices for common use cases MLOps Integration: Apply SageMaker\u0026rsquo;s MLOps capabilities to automate model training and deployment pipelines Safety Implementation: Integrate Bedrock Guardrails into any generative AI applications to ensure content safety Personal Experience This workshop provided an excellent hands-on introduction to AWS AI/ML services:\nThe SageMaker Studio demo was particularly impressive, showing how a unified platform can streamline the entire ML workflow Learning about RAG architecture was eye-opening, as it demonstrated how to build AI applications that can leverage specific knowledge bases The Bedrock Agents demonstration showed the potential for building sophisticated AI applications that can handle complex workflows The practical focus on prompt engineering provided immediately applicable skills for working with language models Understanding Guardrails helped me appreciate the importance of safety and compliance in AI applications Takeaways Start with Use Cases: Always begin by identifying specific business problems before selecting AI/ML solutions Foundation Models are Powerful: Pre-trained foundation models can solve many problems without custom training RAG is Essential: For applications requiring specific knowledge, RAG architecture is the way to go MLOps Matters: Proper MLOps practices are crucial for maintaining ML models in production Safety Cannot be Overlooked: Content filtering and safety measures must be integrated from the start Continuous Learning: The AI/ML landscape evolves rapidly, requiring continuous learning and experimentation Event Photos "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Sharing a vision of a more connected world with AWS IoT AWS IoT VP Yasser Alsaied outlines a strategic shift from horizontal platforms to vertical industry-led solutions designed to deliver specific business outcomes for sectors like automotive and manufacturing. The company commits to continued investment in specialized services (such as AWS IoT SiteWise and FleetWise) and partner networks to simplify deployment from the edge to the cloud. Amazon serves as a major internal customer, validating these technologies at scale through their use in logistics robotics and \u0026ldquo;Just Walk Out\u0026rdquo; retail systems. Future trends focus on IoT becoming a standard business expectation, driving sustainability initiatives and operational efficiency through simplified tools. This approach aims to reduce costs and technical barriers, allowing customers to focus on value rather than infrastructure integration.\nBlog 2 - New and updated courses from AWS Training and Certification in March 2023 In March 2023, AWS Training and Certification released 27 new digital products, featuring 14 AWS Builder Labs and specialized courses for partners and executives. The AWS Skill Builder platform added free courses covering technical topics such as Robotics, IoT, .NET workloads, and data migration, along with strategic content tailored for CFOs and CIOs. In addition to expanding the Team subscription to 50 countries, AWS announced the temporary removal of exam labs from the AWS Certified SysOps Administrator – Associate exam to improve the assessment experience.\nBlog 3 - Hannover Messe: Discover an end-to-end industrial data strategy with AWS At Hannover Messe 2023, AWS highlighted a comprehensive industrial data strategy through the Industrial Data Fabric (IDF) architecture, designed to help manufacturers unify disconnected data sources and scale digital transformation from proof-of-concept to production. In collaboration with strategic partners like Bosch, Siemens, and MHP, AWS demonstrated these capabilities through interactive demos covering the automotive manufacturing value chain, smart supply chains, and carbon footprint tracking. Beyond the exhibits, attendees had access to over 50 deep-dive presentations on industrial automation and the opportunity to book 1:1 consultations with AWS experts to address specific business challenges.\n"
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives (per CloudJourney roadmap) Explore advanced compute: Containers with ECS/EKS and serverless with Lambda (Module 4). Introduce relational databases: RDS setup, multi-AZ, backups (Databases in Module 1 Explore). Integrate services with prior networking and storage. Enhance monitoring with CloudWatch basics. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 4 — Containers intro: Study ECS task definitions, Fargate launch types; deploy simple container app. 22/09/2025 22/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 4 Tuesday Module 4 — Labs: Create ECS cluster, run tasks with ECR images, integrate with ALB. 23/09/2025 23/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 4 Labs Wednesday Module 4 — Serverless: Lambda functions, triggers from S3/EC2, API Gateway integration. 24/09/2025 24/09/2025 https://cloudjourney.awsstudygroup.com/ - Module 4 Thursday Databases — RDS: Provision RDS instance (MySQL/PostgreSQL), connect from EC2, enable multi-AZ. 25/09/2025 25/09/2025 https://cloudjourney.awsstudygroup.com/ - Explore AWS Services Friday Monitoring basics: Set up CloudWatch alarms for EC2 CPU, RDS storage; log Lambda invocations. 26/09/2025 26/09/2025 https://cloudjourney.awsstudygroup.com/ - Monitoring Results \u0026amp; Achievements (Week 3) Containerized app deployed: ECS Fargate cluster running web app, scaled with ALB, images pushed to ECR. Serverless workflow established: Lambda triggered by S3 uploads, integrated with API Gateway for REST endpoints. RDS operational: Multi-AZ MySQL instance created, connected via EC2 app, automated snapshots configured. Monitoring implemented: CloudWatch dashboards for key metrics, alarms notified via SNS. Integration demo prepared: Simple app stack with EC2-Lambda-RDS-CloudWatch. Issues Encountered \u0026amp; Mitigations ECS task failed to pull image → verified ECR permissions and VPC endpoint setup. Lambda cold starts impacted performance → optimized code and increased memory allocation. RDS connection timeout → updated security groups to allow EC2 subnet traffic. Next Steps (Week 4) Deepen databases: NoSQL with DynamoDB, caching with ElastiCache. Advance monitoring: CloudTrail for auditing, X-Ray for tracing. Start migration planning from Module 2. Document integration patterns in Hugo. "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Implement Cognito Authentication",
	"tags": [],
	"description": "",
	"content": "Implement Cognito Authentication In this section, you will set up Amazon Cognito for user authentication and integrate it with API Gateway.\nStep 1: Create Cognito User Pool # cognito/main.tf resource \u0026#34;aws_cognito_user_pool\u0026#34; \u0026#34;main\u0026#34; { name = \u0026#34;secure-serverless-users\u0026#34; # Password policy password_policy { minimum_length = 8 require_lowercase = true require_numbers = true require_symbols = true require_uppercase = true } # MFA configuration mfa_configuration = \u0026#34;OPTIONAL\u0026#34; software_token_mfa_configuration { enabled = true } # Account recovery account_recovery_setting { recovery_mechanism { name = \u0026#34;verified_email\u0026#34; priority = 1 } } # Email configuration auto_verified_attributes = [\u0026#34;email\u0026#34;] # Schema attributes schema { name = \u0026#34;email\u0026#34; attribute_data_type = \u0026#34;String\u0026#34; required = true mutable = true } schema { name = \u0026#34;name\u0026#34; attribute_data_type = \u0026#34;String\u0026#34; required = true mutable = true } # Verification message verification_message_template { default_email_option = \u0026#34;CONFIRM_WITH_CODE\u0026#34; email_subject = \u0026#34;Your verification code\u0026#34; email_message = \u0026#34;Your verification code is {####}\u0026#34; } tags = { Environment = var.environment Project = \u0026#34;secure-serverless\u0026#34; } } Step 2: Create via AWS Console Navigate to Cognito Console → Create user pool Configure sign-in experience: Sign-in options: Email User name requirements: As needed Configure security requirements:\nPassword policy: Custom (8 chars, mixed case, numbers, symbols) MFA: Optional Configure sign-up experience:\nSelf-registration: Enabled Required attributes: email, name Step 3: Create App Client resource \u0026#34;aws_cognito_user_pool_client\u0026#34; \u0026#34;main\u0026#34; { name = \u0026#34;secure-serverless-client\u0026#34; user_pool_id = aws_cognito_user_pool.main.id # Auth flows explicit_auth_flows = [ \u0026#34;ALLOW_USER_PASSWORD_AUTH\u0026#34;, \u0026#34;ALLOW_REFRESH_TOKEN_AUTH\u0026#34;, \u0026#34;ALLOW_USER_SRP_AUTH\u0026#34; ] # Token validity access_token_validity = 1 # hours id_token_validity = 1 # hours refresh_token_validity = 30 # days token_validity_units { access_token = \u0026#34;hours\u0026#34; id_token = \u0026#34;hours\u0026#34; refresh_token = \u0026#34;days\u0026#34; } # OAuth settings allowed_oauth_flows = [\u0026#34;code\u0026#34;, \u0026#34;implicit\u0026#34;] allowed_oauth_scopes = [\u0026#34;email\u0026#34;, \u0026#34;openid\u0026#34;, \u0026#34;profile\u0026#34;] supported_identity_providers = [\u0026#34;COGNITO\u0026#34;] callback_urls = [ \u0026#34;https://your-app.com/callback\u0026#34;, \u0026#34;http://localhost:3000/callback\u0026#34; # For development ] logout_urls = [ \u0026#34;https://your-app.com/logout\u0026#34;, \u0026#34;http://localhost:3000/logout\u0026#34; ] # Prevent user existence errors prevent_user_existence_errors = \u0026#34;ENABLED\u0026#34; # Generate client secret generate_secret = false # Set to true for server-side apps } Step 4: Create Domain resource \u0026#34;aws_cognito_user_pool_domain\u0026#34; \u0026#34;main\u0026#34; { domain = \u0026#34;secure-serverless-${random_string.suffix.result}\u0026#34; user_pool_id = aws_cognito_user_pool.main.id } resource \u0026#34;random_string\u0026#34; \u0026#34;suffix\u0026#34; { length = 8 special = false upper = false } Step 5: Configure API Gateway Authorizer resource \u0026#34;aws_api_gateway_authorizer\u0026#34; \u0026#34;cognito\u0026#34; { name = \u0026#34;cognito-authorizer\u0026#34; rest_api_id = aws_api_gateway_rest_api.main.id type = \u0026#34;COGNITO_USER_POOLS\u0026#34; provider_arns = [aws_cognito_user_pool.main.arn] identity_source = \u0026#34;method.request.header.Authorization\u0026#34; } # Update API method to use authorizer resource \u0026#34;aws_api_gateway_method\u0026#34; \u0026#34;protected\u0026#34; { rest_api_id = aws_api_gateway_rest_api.main.id resource_id = aws_api_gateway_resource.items.id http_method = \u0026#34;GET\u0026#34; authorization = \u0026#34;COGNITO_USER_POOLS\u0026#34; authorizer_id = aws_api_gateway_authorizer.cognito.id } Step 6: Test Authentication 6.1 Create Test User # Create user aws cognito-idp admin-create-user \\ --user-pool-id YOUR_USER_POOL_ID \\ --username testuser@example.com \\ --user-attributes Name=email,Value=testuser@example.com Name=name,Value=\u0026#34;Test User\u0026#34; \\ --temporary-password \u0026#34;TempPass123!\u0026#34; # Set permanent password aws cognito-idp admin-set-user-password \\ --user-pool-id YOUR_USER_POOL_ID \\ --username testuser@example.com \\ --password \u0026#34;SecurePass123!\u0026#34; \\ --permanent 6.2 Authenticate and Get Token # Initiate auth aws cognito-idp initiate-auth \\ --client-id YOUR_CLIENT_ID \\ --auth-flow USER_PASSWORD_AUTH \\ --auth-parameters USERNAME=testuser@example.com,PASSWORD=SecurePass123! The response will include:\nIdToken: JWT for API authentication AccessToken: JWT for user information RefreshToken: For obtaining new tokens 6.3 Call Protected API # Use the IdToken to call the API curl -X GET https://your-api-id.execute-api.ap-southeast-1.amazonaws.com/prod/items \\ -H \u0026#34;Authorization: Bearer YOUR_ID_TOKEN\u0026#34; Step 7: Lambda Token Verification import boto3 from jose import jwt, JWTError import requests import os def verify_token(token): \u0026#34;\u0026#34;\u0026#34;Verify Cognito JWT token\u0026#34;\u0026#34;\u0026#34; region = os.environ[\u0026#39;AWS_REGION\u0026#39;] user_pool_id = os.environ[\u0026#39;USER_POOL_ID\u0026#39;] client_id = os.environ[\u0026#39;CLIENT_ID\u0026#39;] # Get JWKS jwks_url = f\u0026#39;https://cognito-idp.{region}.amazonaws.com/{user_pool_id}/.well-known/jwks.json\u0026#39; jwks = requests.get(jwks_url).json() try: # Decode and verify token claims = jwt.decode( token, jwks, algorithms=[\u0026#39;RS256\u0026#39;], audience=client_id, issuer=f\u0026#39;https://cognito-idp.{region}.amazonaws.com/{user_pool_id}\u0026#39; ) return claims except JWTError as e: raise Exception(f\u0026#39;Token verification failed: {str(e)}\u0026#39;) Never expose your User Pool ID or Client ID in client-side code without proper security measures. Use environment variables and secrets management.\nImages Required: cognito-flow.png - Cognito authentication flow diagram create-user-pool.png - User Pool creation in console app-client.png - App client configuration api-auth-test.png - Testing authenticated API calls "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/5-workshop/5.3-s3-vpc/",
	"title": "Setting up CI/CD Pipeline",
	"tags": [],
	"description": "",
	"content": "Setting up CI/CD Pipeline In this section, you will set up a CI/CD pipeline using AWS CodePipeline and CodeBuild to automate building, scanning, and deploying your FastAPI application.\nPipeline Overview The CI/CD pipeline consists of the following stages:\nStage Tool Purpose Lint Semgrep Static Application Security Testing (SAST) Build CodeBuild + Docker Build container image Scan Trivy Scan filesystem and container for vulnerabilities Push ECR Store container image Deploy Terraform Provision/update infrastructure Content Create CodeBuild Projects Configure Security Scanning Pipeline Flow ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ │ Git Push │───▶│ CodeBuild │───▶│ CodeBuild │ │ (Source) │ │ (Build) │ │ (Deploy) │ └─────────────┘ └─────────────┘ └─────────────┘ │ ┌────────────────┼────────────────┐ ▼ ▼ ▼ ┌──────────┐ ┌──────────┐ ┌──────────┐ │ Semgrep │ │ Docker │ │ Trivy │ │ (SAST) │ │ Build │ │ (Scan) │ └──────────┘ └──────────┘ └──────────┘ │ ▼ ┌──────────┐ │ ECR │ │ (Push) │ └──────────┘ │ ▼ ┌──────────┐ │Terraform │ │ (Deploy) │ └──────────┘ Build Stage - buildspec-build.yml This is the actual buildspec file from the project:\nversion: 0.2 env: variables: IMAGE_NAME: \u0026#34;fastapi-lambda\u0026#34; IMAGE_TAG: \u0026#34;latest\u0026#34; phases: install: runtime-versions: docker: 20 commands: - pip3 install semgrep trivy pre_build: commands: - echo \u0026#34;Running Semgrep...\u0026#34; - semgrep --config backend/semgrep.yml || exit 1 build: commands: - echo \u0026#34;Docker build...\u0026#34; - docker build -t ${IMAGE_NAME}:${IMAGE_TAG} backend - echo \u0026#34;Scanning filesystem with Trivy...\u0026#34; - trivy fs --exit-code 1 --severity HIGH,CRITICAL --ignorefile backend/trivyignore.txt . || exit 1 - echo \u0026#34;Scanning image with Trivy...\u0026#34; - trivy image --exit-code 1 --severity HIGH,CRITICAL ${IMAGE_NAME}:${IMAGE_TAG} || exit 1 post_build: commands: - echo \u0026#34;Logging in to ECR...\u0026#34; - aws ecr get-login-password | docker login --username AWS --password-stdin ${ECR_URI} - docker tag ${IMAGE_NAME}:${IMAGE_TAG} ${ECR_URI}/${IMAGE_NAME}:${IMAGE_TAG} - docker push ${ECR_URI}/${IMAGE_NAME}:${IMAGE_TAG} artifacts: files: - image-detail.json cache: paths: - \u0026#39;/root/.cache/semgrep/**\u0026#39; - \u0026#39;/root/.cache/trivy/**\u0026#39; Deploy Stage - buildspec-deploy.yml version: 0.2 phases: install: commands: - curl -fsSL https://releases.hashicorp.com/terraform/1.9.5/terraform_1.9.5_linux_amd64.zip -o tf.zip - unzip -o tf.zip -d /usr/local/bin - terraform -v pre_build: commands: - cd infra - terraform init -upgrade build: commands: - terraform plan -input=false -out=tfplan - terraform apply -auto-approve tfplan post_build: commands: - echo \u0026#34;Deployment finished\u0026#34; Semgrep Configuration The project uses custom Semgrep rules for security scanning:\n# backend/semgrep.yml rules: - id: jwt-hardcoded-secret message: Avoid hardcoding JWT secrets; use Secrets Manager/env. severity: ERROR languages: [python] pattern: | jwt.encode($P, $S, ...) metavariable-pattern: metavariable: $S pattern: \u0026#34;\u0026#39;$SECRET\u0026#39;\u0026#34; - id: fastapi-debug message: Do not run uvicorn with reload or debug in production. severity: WARNING languages: [python] pattern-either: - pattern: uvicorn.run(..., reload=True, ...) - pattern: uvicorn.run(..., debug=True, ...) Dockerfile for Lambda Container # backend/Dockerfile # Lambda base image for Python 3.10 FROM public.ecr.aws/lambda/python:3.10 # Copy and install requirements COPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt -t \u0026#34;${LAMBDA_TASK_ROOT}\u0026#34; # Copy application code COPY app/ ${LAMBDA_TASK_ROOT}/app/ # Set the handler CMD [\u0026#34;app.lambda_handler.handler\u0026#34;] Key Dependencies (requirements.txt) fastapi\u0026gt;=0.100.0 mangum\u0026gt;=0.17.0 boto3\u0026gt;=1.28.0 pydantic\u0026gt;=2.0.0 python-jose[cryptography]\u0026gt;=3.3.0 passlib[bcrypt]\u0026gt;=1.7.4 uvicorn\u0026gt;=0.23.0 The pipeline uses Semgrep for static code analysis and Trivy for both filesystem and container image scanning. This provides defense-in-depth security scanning at multiple stages.\nImages Required: cicd-architecture.png - CI/CD pipeline architecture codebuild-console.png - CodeBuild project console ecr-repository.png - ECR repository with images pipeline-success.png - Successful pipeline execution "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "DevOps on AWS Workshop Event Information Date \u0026amp; Time: Monday, November 17, 2025, 8:30 AM – 5:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent Purpose The workshop was designed to provide comprehensive knowledge and hands-on experience with AWS DevOps services, covering CI/CD pipelines, Infrastructure as Code, container services, and monitoring \u0026amp; observability. The event aimed to help participants understand DevOps culture, principles, and best practices while exploring practical implementation of DevOps workflows on AWS.\nAgenda Overview Morning Session (8:30 AM – 12:00 PM) 8:30 – 9:00 AM | Welcome \u0026amp; DevOps Mindset\nRecap of AI/ML session from previous workshop DevOps Culture and Principles: Understanding the cultural shift from traditional IT to DevOps, emphasizing collaboration, automation, and continuous improvement Benefits and Key Metrics: DORA Metrics: Deployment frequency, lead time for changes, mean time to recovery (MTTR), change failure rate MTTR (Mean Time To Recovery): Measuring how quickly teams can recover from failures Deployment Frequency: Tracking how often teams deploy code to production Discussion on how DevOps practices improve software delivery and operational performance 9:00 – 10:30 AM | AWS DevOps Services – CI/CD Pipeline\nSource Control: AWS CodeCommit, Git Strategies\nAWS CodeCommit: Fully managed source control service, secure Git repositories Git Strategies: GitFlow: Feature branches, develop, release branches workflow Trunk-based Development: Main branch focused, short-lived feature branches Best practices for branching strategies based on team size and project requirements Build \u0026amp; Test: CodeBuild Configuration, Testing Pipelines\nAWS CodeBuild: Fully managed build service that compiles source code, runs tests, and produces ready-to-deploy software packages Build Configuration: Buildspec files, environment variables, and build artifacts Testing Pipelines: Unit tests, integration tests, and automated test execution Integration with testing frameworks and code quality tools Deployment: CodeDeploy with Blue/Green, Canary, and Rolling Updates\nAWS CodeDeploy: Automated application deployments to EC2, Lambda, or on-premises servers Blue/Green Deployment: Zero-downtime deployment by running two identical production environments Canary Deployment: Gradual rollout to a small percentage of users before full deployment Rolling Updates: Incremental deployment across instances with automatic rollback capabilities Choosing the right deployment strategy based on application requirements Orchestration: CodePipeline Automation\nAWS CodePipeline: Fully managed continuous delivery service for automating release pipelines Pipeline Stages: Source, Build, Test, Deploy, and Approval stages Integration: Connecting CodeCommit, CodeBuild, CodeDeploy, and other AWS services Automation: Automated triggers, parallel execution, and pipeline visualization Demo: Full CI/CD Pipeline Walkthrough\nThe demonstration showcased a complete CI/CD pipeline:\nSetting up CodeCommit repository Configuring CodeBuild for automated builds and tests Creating CodeDeploy application with Blue/Green deployment Building CodePipeline to orchestrate the entire workflow Testing the pipeline with code changes and observing automated deployment 10:30 – 10:45 AM | Break\nNetworking and refreshments.\n10:45 AM – 12:00 PM | Infrastructure as Code (IaC)\nAWS CloudFormation: Templates, Stacks, and Drift Detection\nCloudFormation Templates: JSON/YAML templates for defining AWS resources Stacks: Collections of AWS resources managed as a single unit Drift Detection: Identifying changes made outside of CloudFormation Stack Updates: Updating infrastructure with change sets and rollback capabilities Best Practices: Template organization, parameterization, and nested stacks AWS CDK (Cloud Development Kit): Constructs, Reusable Patterns, and Language Support\nAWS CDK: Define cloud infrastructure using familiar programming languages (TypeScript, Python, Java, C#, Go) Constructs: Reusable cloud components, from low-level resources to high-level patterns Reusable Patterns: Pre-built solutions for common use cases (VPC, ECS clusters, serverless applications) Language Support: TypeScript, Python, Java, C#, Go, and JavaScript Benefits: Type safety, IDE support, and easier testing compared to CloudFormation templates Demo: Deploying with CloudFormation and CDK\nThe demonstration compared both approaches:\nCloudFormation: Deploying a VPC and EC2 instance using YAML template CDK: Same infrastructure using TypeScript with CDK constructs Highlighting the differences in approach, maintainability, and developer experience Discussion: Choosing between IaC Tools\nWhen to use CloudFormation vs CDK Considerations: team expertise, project complexity, and maintenance requirements Hybrid approaches: Using both tools together for different parts of infrastructure Lunch Break (12:00 – 1:00 PM)\nSelf-arranged lunch break.\nAfternoon Session (1:00 – 5:00 PM) 1:00 – 2:30 PM | Container Services on AWS\nDocker Fundamentals: Microservices and Containerization\nContainerization Concepts: Understanding containers, images, and containerization benefits Microservices Architecture: Breaking monolithic applications into smaller, independent services Docker Basics: Dockerfile, image building, container lifecycle Benefits: Portability, consistency, and resource efficiency Amazon ECR: Image Storage, Scanning, Lifecycle Policies\nAmazon ECR: Fully managed Docker container registry Image Storage: Secure, scalable storage for Docker images Image Scanning: Automated vulnerability scanning for container images Lifecycle Policies: Automating image cleanup and retention policies Integration: Seamless integration with ECS, EKS, and other AWS services Amazon ECS \u0026amp; EKS: Deployment Strategies, Scaling, and Orchestration\nAmazon ECS: Fully managed container orchestration service\nTask Definitions: Container specifications, resource requirements, and networking Services: Long-running tasks with load balancing and auto-scaling Deployment Strategies: Rolling updates, Blue/Green deployments Scaling: Auto-scaling based on CPU, memory, or custom metrics Amazon EKS: Managed Kubernetes service\nKubernetes Concepts: Pods, Services, Deployments, and Namespaces EKS Features: Managed control plane, node groups, and add-ons Deployment Strategies: Rolling updates, Canary deployments with Istio/App Mesh Scaling: Cluster autoscaler and horizontal pod autoscaler AWS App Runner: Simplified Container Deployment\nApp Runner: Fully managed service for building and running containerized applications Simplified Deployment: Deploy from source code or container image Auto-scaling: Automatic scaling based on traffic Use Cases: Web applications, APIs, and microservices Comparison: When to use App Runner vs ECS vs EKS Demo \u0026amp; Case Study: Microservices Deployment Comparison\nThe demonstration compared different container deployment options:\nDeploying a simple web application with App Runner Same application with ECS Fargate Comparison of setup complexity, cost, and operational overhead Case study: Choosing the right container service for different scenarios 2:30 – 2:45 PM | Break\nNetworking and refreshments.\n2:45 – 4:00 PM | Monitoring \u0026amp; Observability\nCloudWatch: Metrics, Logs, Alarms, and Dashboards\nCloudWatch Metrics: Collecting and tracking metrics from AWS services and custom applications CloudWatch Logs: Centralized logging, log groups, and log streams CloudWatch Alarms: Automated actions based on metric thresholds CloudWatch Dashboards: Customizable dashboards for visualizing metrics and logs Best Practices: Metric naming conventions, log retention, and alarm configuration AWS X-Ray: Distributed Tracing and Performance Insights\nAWS X-Ray: Service for analyzing and debugging distributed applications Distributed Tracing: End-to-end request tracing across microservices Service Map: Visual representation of application architecture and dependencies Performance Insights: Identifying bottlenecks and performance issues Integration: X-Ray SDK integration with applications and AWS services Demo: Full-Stack Observability Setup\nThe demonstration showed:\nSetting up CloudWatch metrics and logs for an application Creating CloudWatch dashboards for monitoring Configuring CloudWatch alarms for alerting Enabling X-Ray tracing for distributed tracing Viewing service maps and trace analysis Best Practices: Alerting, Dashboards, and On-Call Processes\nAlerting Strategy: Setting up meaningful alerts, avoiding alert fatigue Dashboard Design: Creating effective dashboards for different audiences (developers, operations, management) On-Call Processes: Incident response procedures, escalation paths, and runbooks SLO/SLI: Service Level Objectives and Indicators for measuring reliability 4:00 – 4:45 PM | DevOps Best Practices \u0026amp; Case Studies\nDeployment Strategies: Feature Flags, A/B Testing\nFeature Flags: Gradual feature rollouts, canary releases, and instant rollbacks A/B Testing: Comparing different versions to optimize user experience Tools: AWS AppConfig, LaunchDarkly integration Best Practices: Feature flag management and testing strategies Automated Testing and CI/CD Integration\nTesting Pyramid: Unit tests, integration tests, and end-to-end tests Test Automation: Automated test execution in CI/CD pipelines Quality Gates: Blocking deployments based on test results Test Coverage: Measuring and improving test coverage Incident Management and Postmortems\nIncident Response: Detection, response, and recovery procedures Postmortems: Learning from incidents, documenting root causes Blameless Culture: Focusing on system improvements rather than individual blame Tools: Incident management tools and communication channels Case Studies: Startups and Enterprise DevOps Transformations\nStartup Case Study: Rapid scaling with DevOps practices, cost optimization Enterprise Case Study: Large-scale migration to DevOps, cultural transformation Lessons Learned: Common challenges and solutions ROI: Measuring the impact of DevOps adoption 4:45 – 5:00 PM | Q\u0026amp;A \u0026amp; Wrap-up\nDevOps Career Pathways: Career progression in DevOps, required skills AWS Certification Roadmap: Relevant AWS certifications for DevOps engineers AWS Certified DevOps Engineer – Professional AWS Certified Solutions Architect AWS Certified SysOps Administrator Next Steps: Resources for continued learning and practice Closing Remarks: Summary of key takeaways and action items Key Highlights CI/CD Pipeline: AWS CodePipeline provides a complete solution for automating software delivery from source to production Infrastructure as Code: Both CloudFormation and CDK offer powerful ways to manage infrastructure, with CDK providing better developer experience Container Services: AWS offers multiple container options (ECS, EKS, App Runner) for different use cases and complexity levels Observability: CloudWatch and X-Ray together provide comprehensive monitoring and tracing capabilities DevOps Culture: Success requires cultural change, not just tools and technology Best Practices: Feature flags, automated testing, and incident management are essential for modern DevOps Key Learnings DevOps is Culture First: Tools are important, but cultural transformation is the foundation of DevOps success CI/CD Automation: Automating the entire software delivery pipeline significantly improves speed and reliability IaC Benefits: Infrastructure as Code enables version control, repeatability, and faster infrastructure changes Container Strategy: Choosing the right container service depends on complexity, team expertise, and operational requirements Observability is Critical: Comprehensive monitoring and tracing are essential for maintaining reliable systems Continuous Improvement: DevOps is about continuous learning and improvement, not a one-time implementation Application to My Work Implement CI/CD: Set up CodePipeline for automated deployments in current projects Adopt IaC: Start using CloudFormation or CDK for infrastructure management Container Migration: Evaluate containerization opportunities for existing applications Improve Monitoring: Enhance CloudWatch dashboards and alarms for better visibility Practice DevOps: Apply DevOps principles and practices in daily work Incident Management: Establish incident response procedures and postmortem practices Personal Experience This full-day DevOps workshop was comprehensive and highly practical:\nThe CI/CD pipeline demonstration was particularly valuable, showing how to automate the entire software delivery process Learning about different IaC tools helped me understand when to use CloudFormation vs CDK The container services comparison provided clear guidance on choosing the right service for different scenarios The observability session emphasized the importance of monitoring and tracing for maintaining reliable systems The case studies provided real-world insights into DevOps transformations The career pathway discussion was motivating and provided clear direction for professional development Takeaways Start Small: Begin with basic CI/CD automation and gradually expand DevOps practices Culture Matters: DevOps success requires team collaboration and cultural change Choose the Right Tools: Select tools based on team expertise and project requirements Monitor Everything: Comprehensive observability is essential for reliable systems Learn Continuously: DevOps practices evolve rapidly, requiring continuous learning Measure Success: Use DORA metrics to track DevOps improvements and ROI Event Photos "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives (per CloudJourney roadmap) Expand databases: NoSQL DynamoDB, caching layers (Databases in Module 1). Enhance observability: Auditing with CloudTrail, tracing with X-Ray (Monitoring). Initial migration assessment tools (Transition to Module 2). Consolidate foundational services. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Databases — NoSQL: Design DynamoDB tables, provisioned vs on-demand capacity, global tables. 29/09/2025 29/09/2025 https://cloudjourney.awsstudygroup.com/ - Explore AWS Services Tuesday Labs: Create DynamoDB table, load data via Lambda, query with PartiQL. 30/09/2025 30/09/2025 https://cloudjourney.awsstudygroup.com/ - Databases Labs Wednesday Caching: Set up ElastiCache Redis, integrate with RDS/DynamoDB for read-heavy apps. 01/10/2025 01/10/2025 https://cloudjourney.awsstudygroup.com/ - Databases Thursday Monitoring advanced: Enable CloudTrail trails, integrate X-Ray with Lambda/ECS. 02/10/2025 02/10/2025 https://cloudjourney.awsstudygroup.com/ - Monitoring Friday Module 2 intro: Workload assessment using Migration Evaluator, plan simple lift-and-shift. 03/10/2025 03/10/2025 https://cloudjourney.awsstudygroup.com/ - Migrate to AWS Results \u0026amp; Achievements (Week 4) DynamoDB mastered: Tables created with secondary indexes, data ingested, queries optimized for performance. Caching layer added: Redis cluster reducing RDS load by 70% in test scenarios. Observability stack complete: CloudTrail logging API calls, X-Ray traces showing end-to-end latencies. Migration planning initiated: Assessed sample on-prem workload, identified EC2 equivalents. Foundational services consolidated: Full stack (network-storage-compute-db-monitoring) documented. Issues Encountered \u0026amp; Mitigations DynamoDB provisioned capacity over-provisioned → switched to on-demand and monitored with CloudWatch. X-Ray sampling rate too high → adjusted to 10% for cost control. Migration Evaluator data upload slow → used direct connector for faster assessment. Next Steps (Week 5) Dive into Module 2: Server and application migration strategies. Database migration with DMS. Network connectivity for hybrid setups. Review foundational knowledge with a mini-project. "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025: Ho Chi Minh Connect Edition for Builders\nDate \u0026amp; Time: 08:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GAI-Driven Development Life Cycle: Reimagining Software Engineering.\nDate \u0026amp; Time: 14:00, October 3, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AI/ML/GenAI on AWS Workshop\nDate \u0026amp; Time: Saturday, November 15, 2025, 8:30 AM – 12:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent 4 Event Name: DevOps on AWS Workshop\nDate \u0026amp; Time: Monday, November 17, 2025, 8:30 AM – 5:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent 5 Event Name: AWS Well-Architected Security Pillar Workshop\nDate \u0026amp; Time: Saturday, November 29, 2025, 8:30 AM – 12:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/5-workshop/5.4-s3-onprem/",
	"title": "Building FastAPI Backend",
	"tags": [],
	"description": "",
	"content": "Building FastAPI Backend In this section, you will build the FastAPI backend with layered architecture: API → Service → Repository, and deploy it to AWS Lambda.\nArchitecture Overview The application follows a clean layered architecture:\nLayer Location Responsibility API app/api/routers/ HTTP request handling, routing Service app/services/ Business logic, orchestration Repository app/repositories/ Data access, DynamoDB operations Models app/models/ Data structures, validation Core app/core/ Configuration, security, logging Request Flow HTTP Request ↓ app/main.py (FastAPI App) ↓ app/api/routers/*.py (Endpoints) ↓ app/services/*.py (Business Logic) ↓ app/repositories/*.py (DynamoDB) ↓ DynamoDB Tables Content FastAPI Application Structure DynamoDB Tables \u0026amp; Repository JWT Authentication Testing the API Main FastAPI Application # backend/app/main.py from fastapi import FastAPI from app.api.routers.auth import router as auth_router from app.api.routers.products import router as products_router from app.api.routers.orders import router as orders_router from app.core.logging import configure_logging from app.api.middleware import add_correlation_middleware def create_app() -\u0026gt; FastAPI: configure_logging() app = FastAPI(title=\u0026#34;Products \u0026amp; Orders API\u0026#34;, version=\u0026#34;1.0.0\u0026#34;) add_correlation_middleware(app) @app.get(\u0026#34;/health\u0026#34;, tags=[\u0026#34;health\u0026#34;]) async def health() -\u0026gt; dict: return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;} app.include_router(auth_router, prefix=\u0026#34;/auth\u0026#34;, tags=[\u0026#34;auth\u0026#34;]) app.include_router(products_router, prefix=\u0026#34;/products\u0026#34;, tags=[\u0026#34;products\u0026#34;]) app.include_router(orders_router, prefix=\u0026#34;/orders\u0026#34;, tags=[\u0026#34;orders\u0026#34;]) return app app = create_app() Lambda Handler with Mangum # backend/app/lambda_handler.py from mangum import Mangum from app.main import app # Expose Lambda handler for API Gateway HTTP API handler = Mangum(app) Products Router Example # backend/app/api/routers/products.py from fastapi import APIRouter, Depends, HTTPException, status, Query from typing import Optional, List from app.api.deps import require_admin, get_product_service from app.services.product_service import ProductService from app.models.product import ProductCreate, ProductUpdate, ProductOut router = APIRouter() @router.post(\u0026#34;\u0026#34;, response_model=ProductOut, status_code=status.HTTP_201_CREATED, dependencies=[Depends(require_admin)]) async def create_product(payload: ProductCreate, service: ProductService = Depends(get_product_service)) -\u0026gt; ProductOut: return await service.create_product(payload) @router.get(\u0026#34;\u0026#34;, response_model=List[ProductOut]) async def list_products(category: Optional[str] = Query(default=None), service: ProductService = Depends(get_product_service)) -\u0026gt; List[ProductOut]: return await service.list_products(category=category) @router.get(\u0026#34;/{product_id}\u0026#34;, response_model=ProductOut) async def get_product(product_id: str, service: ProductService = Depends(get_product_service)) -\u0026gt; ProductOut: product = await service.get_product(product_id) if not product: raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=\u0026#34;Product not found\u0026#34;) return product @router.put(\u0026#34;/{product_id}\u0026#34;, response_model=ProductOut, dependencies=[Depends(require_admin)]) async def update_product(product_id: str, payload: ProductUpdate, service: ProductService = Depends(get_product_service)) -\u0026gt; ProductOut: return await service.update_product(product_id, payload) @router.delete(\u0026#34;/{product_id}\u0026#34;, status_code=status.HTTP_204_NO_CONTENT, dependencies=[Depends(require_admin)]) async def delete_product(product_id: str, service: ProductService = Depends(get_product_service)) -\u0026gt; None: await service.delete_product(product_id) DynamoDB Repository # backend/app/repositories/ddb_products.py import boto3 import uuid from decimal import Decimal from typing import Optional, List from app.models.product import ProductCreate, ProductUpdate, ProductOut from app.utils.time import utcnow_iso class DynamoProductsRepository: def __init__(self, region: str, table_name: str) -\u0026gt; None: self._table = boto3.resource(\u0026#34;dynamodb\u0026#34;, region_name=region).Table(table_name) async def create(self, payload: ProductCreate) -\u0026gt; ProductOut: product_id = str(uuid.uuid4()) item = { \u0026#34;product_id\u0026#34;: product_id, \u0026#34;name\u0026#34;: payload.name, \u0026#34;price\u0026#34;: Decimal(str(payload.price)), \u0026#34;stock\u0026#34;: payload.stock, \u0026#34;category\u0026#34;: payload.category or \u0026#34;\u0026#34;, \u0026#34;updated_at\u0026#34;: utcnow_iso(), } self._table.put_item(Item=item) return ProductOut(**item) async def get(self, product_id: str) -\u0026gt; Optional[ProductOut]: res = self._table.get_item(Key={\u0026#34;product_id\u0026#34;: product_id}) item = res.get(\u0026#34;Item\u0026#34;) return ProductOut(**item) if item else None async def list(self, category: Optional[str]) -\u0026gt; List[ProductOut]: if category: res = self._table.query( IndexName=\u0026#34;category-index\u0026#34;, KeyConditionExpression=boto3.dynamodb.conditions.Key(\u0026#34;category\u0026#34;).eq(category), ) else: res = self._table.scan() return [ProductOut(**i) for i in res.get(\u0026#34;Items\u0026#34;, [])] async def delete(self, product_id: str) -\u0026gt; None: self._table.delete_item(Key={\u0026#34;product_id\u0026#34;: product_id}) Terraform Infrastructure # infra/modules/dynamodb/main.tf resource \u0026#34;aws_dynamodb_table\u0026#34; \u0026#34;products\u0026#34; { name = var.products_table_name billing_mode = \u0026#34;PAY_PER_REQUEST\u0026#34; hash_key = \u0026#34;product_id\u0026#34; attribute { name = \u0026#34;product_id\u0026#34; type = \u0026#34;S\u0026#34; } attribute { name = \u0026#34;category\u0026#34; type = \u0026#34;S\u0026#34; } global_secondary_index { name = \u0026#34;category-index\u0026#34; hash_key = \u0026#34;category\u0026#34; projection_type = \u0026#34;ALL\u0026#34; } } resource \u0026#34;aws_dynamodb_table\u0026#34; \u0026#34;orders\u0026#34; { name = var.orders_table_name billing_mode = \u0026#34;PAY_PER_REQUEST\u0026#34; hash_key = \u0026#34;order_id\u0026#34; attribute { name = \u0026#34;order_id\u0026#34; type = \u0026#34;S\u0026#34; } attribute { name = \u0026#34;user_id\u0026#34; type = \u0026#34;S\u0026#34; } global_secondary_index { name = \u0026#34;user-index\u0026#34; hash_key = \u0026#34;user_id\u0026#34; projection_type = \u0026#34;ALL\u0026#34; } } resource \u0026#34;aws_dynamodb_table\u0026#34; \u0026#34;users\u0026#34; { name = var.users_table_name billing_mode = \u0026#34;PAY_PER_REQUEST\u0026#34; hash_key = \u0026#34;user_id\u0026#34; attribute { name = \u0026#34;user_id\u0026#34; type = \u0026#34;S\u0026#34; } attribute { name = \u0026#34;email\u0026#34; type = \u0026#34;S\u0026#34; } global_secondary_index { name = \u0026#34;email-index\u0026#34; hash_key = \u0026#34;email\u0026#34; projection_type = \u0026#34;ALL\u0026#34; } } API Gateway HTTP API # infra/modules/apigw/main.tf resource \u0026#34;aws_apigatewayv2_api\u0026#34; \u0026#34;http\u0026#34; { name = \u0026#34;${var.project_name}-http\u0026#34; protocol_type = \u0026#34;HTTP\u0026#34; } resource \u0026#34;aws_apigatewayv2_integration\u0026#34; \u0026#34;lambda\u0026#34; { api_id = aws_apigatewayv2_api.http.id integration_type = \u0026#34;AWS_PROXY\u0026#34; integration_uri = var.lambda_arn payload_format_version = \u0026#34;2.0\u0026#34; } resource \u0026#34;aws_apigatewayv2_route\u0026#34; \u0026#34;any\u0026#34; { api_id = aws_apigatewayv2_api.http.id route_key = \u0026#34;$default\u0026#34; target = \u0026#34;integrations/${aws_apigatewayv2_integration.lambda.id}\u0026#34; } resource \u0026#34;aws_apigatewayv2_stage\u0026#34; \u0026#34;default\u0026#34; { api_id = aws_apigatewayv2_api.http.id name = \u0026#34;$default\u0026#34; auto_deploy = true } resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;allow_apigw\u0026#34; { statement_id = \u0026#34;AllowExecutionFromAPIGateway\u0026#34; action = \u0026#34;lambda:InvokeFunction\u0026#34; function_name = var.lambda_arn principal = \u0026#34;apigateway.amazonaws.com\u0026#34; source_arn = \u0026#34;${aws_apigatewayv2_api.http.execution_arn}/*/*\u0026#34; } The FastAPI application uses a layered architecture pattern: API layer handles HTTP, Service layer contains business logic, and Repository layer manages data access. This separation makes the code testable and maintainable.\nImages Required: backend-architecture.png - Backend layered architecture dynamodb-tables.png - DynamoDB tables in console api-endpoints.png - FastAPI Swagger documentation lambda-function.png - Lambda function configuration "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "Test and Validate Backend",
	"tags": [],
	"description": "",
	"content": "Test and Validate Backend In this section, you will perform comprehensive testing of the serverless backend to ensure all components work correctly together.\nStep 1: End-to-End Test Plan Test Case Description Expected Result Create Item POST request to create new item 201 Created Get Items GET request to retrieve items 200 OK with items Update Item PUT request to update item 200 OK Delete Item DELETE request to remove item 204 No Content Unauthorized Access Request without token 401 Unauthorized Invalid Token Request with invalid token 403 Forbidden Step 2: Set Up Test Environment # Set environment variables export API_URL=\u0026#34;https://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com/prod\u0026#34; export USER_POOL_ID=\u0026#34;ap-southeast-1_XXXXXXXXX\u0026#34; export CLIENT_ID=\u0026#34;your-client-id\u0026#34; # Get authentication token TOKEN=$(aws cognito-idp initiate-auth \\ --client-id $CLIENT_ID \\ --auth-flow USER_PASSWORD_AUTH \\ --auth-parameters USERNAME=testuser@example.com,PASSWORD=SecurePass123! \\ --query \u0026#39;AuthenticationResult.IdToken\u0026#39; \\ --output text) echo \u0026#34;Token: $TOKEN\u0026#34; Step 3: API Testing with curl 3.1 Create Item curl -X POST $API_URL/items \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;Test Item\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;This is a test item\u0026#34;, \u0026#34;price\u0026#34;: 29.99 }\u0026#39; Expected response:\n{ \u0026#34;message\u0026#34;: \u0026#34;Item created successfully\u0026#34;, \u0026#34;item\u0026#34;: { \u0026#34;PK\u0026#34;: \u0026#34;USER#user-id\u0026#34;, \u0026#34;SK\u0026#34;: \u0026#34;ITEM#2024-01-15T10:30:00Z\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Test Item\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;This is a test item\u0026#34;, \u0026#34;price\u0026#34;: 29.99 } } 3.2 Get All Items curl -X GET $API_URL/items \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; 3.3 Get Single Item curl -X GET \u0026#34;$API_URL/items/ITEM%232024-01-15T10:30:00Z\u0026#34; \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; 3.4 Update Item curl -X PUT \u0026#34;$API_URL/items/ITEM%232024-01-15T10:30:00Z\u0026#34; \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;Updated Item\u0026#34;, \u0026#34;price\u0026#34;: 39.99 }\u0026#39; 3.5 Delete Item curl -X DELETE \u0026#34;$API_URL/items/ITEM%232024-01-15T10:30:00Z\u0026#34; \\ -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; Step 4: Test Authentication Scenarios 4.1 Test Unauthorized Access # Request without token curl -X GET $API_URL/items # Expected: 401 Unauthorized 4.2 Test Invalid Token # Request with invalid token curl -X GET $API_URL/items \\ -H \u0026#34;Authorization: Bearer invalid-token\u0026#34; # Expected: 401 Unauthorized 4.3 Test Expired Token # Wait for token to expire (1 hour), then test curl -X GET $API_URL/items \\ -H \u0026#34;Authorization: Bearer $EXPIRED_TOKEN\u0026#34; # Expected: 401 Unauthorized with message about expired token Step 5: Load Testing with Artillery Create load-test.yml:\nconfig: target: \u0026#34;https://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com/prod\u0026#34; phases: - duration: 60 arrivalRate: 10 name: \u0026#34;Warm up\u0026#34; - duration: 120 arrivalRate: 50 name: \u0026#34;Ramp up\u0026#34; - duration: 60 arrivalRate: 10 name: \u0026#34;Cool down\u0026#34; defaults: headers: Authorization: \u0026#34;Bearer {{ $processEnvironment.TOKEN }}\u0026#34; Content-Type: \u0026#34;application/json\u0026#34; scenarios: - name: \u0026#34;Get items\u0026#34; weight: 70 flow: - get: url: \u0026#34;/items\u0026#34; - name: \u0026#34;Create and delete item\u0026#34; weight: 30 flow: - post: url: \u0026#34;/items\u0026#34; json: name: \u0026#34;Load test item\u0026#34; description: \u0026#34;Created during load test\u0026#34; Run load test:\nexport TOKEN=\u0026#34;your-token\u0026#34; artillery run load-test.yml Step 6: Verify CloudWatch Logs Check Lambda logs for errors:\n# Get recent logs aws logs filter-log-events \\ --log-group-name /aws/lambda/secure-serverless-api \\ --start-time $(date -d \u0026#39;1 hour ago\u0026#39; +%s000) \\ --filter-pattern \u0026#34;ERROR\u0026#34; Step 7: Verify DynamoDB Data # Scan table to verify data aws dynamodb scan \\ --table-name secure-serverless-table \\ --max-items 10 Step 8: Create Automated Test Script #!/usr/bin/env python3 \u0026#34;\u0026#34;\u0026#34; Backend validation test script \u0026#34;\u0026#34;\u0026#34; import requests import json import boto3 import sys API_URL = \u0026#34;https://YOUR_API_ID.execute-api.ap-southeast-1.amazonaws.com/prod\u0026#34; def get_token(): \u0026#34;\u0026#34;\u0026#34;Get Cognito authentication token\u0026#34;\u0026#34;\u0026#34; client = boto3.client(\u0026#39;cognito-idp\u0026#39;) response = client.initiate_auth( ClientId=\u0026#39;your-client-id\u0026#39;, AuthFlow=\u0026#39;USER_PASSWORD_AUTH\u0026#39;, AuthParameters={ \u0026#39;USERNAME\u0026#39;: \u0026#39;testuser@example.com\u0026#39;, \u0026#39;PASSWORD\u0026#39;: \u0026#39;SecurePass123!\u0026#39; } ) return response[\u0026#39;AuthenticationResult\u0026#39;][\u0026#39;IdToken\u0026#39;] def test_create_item(token): \u0026#34;\u0026#34;\u0026#34;Test item creation\u0026#34;\u0026#34;\u0026#34; response = requests.post( f\u0026#34;{API_URL}/items\u0026#34;, headers={\u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {token}\u0026#34;}, json={\u0026#34;name\u0026#34;: \u0026#34;Test\u0026#34;, \u0026#34;price\u0026#34;: 10} ) assert response.status_code == 201, f\u0026#34;Create failed: {response.text}\u0026#34; print(\u0026#34;✅ Create item: PASSED\u0026#34;) return response.json() def test_get_items(token): \u0026#34;\u0026#34;\u0026#34;Test getting items\u0026#34;\u0026#34;\u0026#34; response = requests.get( f\u0026#34;{API_URL}/items\u0026#34;, headers={\u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer {token}\u0026#34;} ) assert response.status_code == 200, f\u0026#34;Get failed: {response.text}\u0026#34; print(\u0026#34;✅ Get items: PASSED\u0026#34;) return response.json() def test_unauthorized(): \u0026#34;\u0026#34;\u0026#34;Test unauthorized access\u0026#34;\u0026#34;\u0026#34; response = requests.get(f\u0026#34;{API_URL}/items\u0026#34;) assert response.status_code == 401, \u0026#34;Should be unauthorized\u0026#34; print(\u0026#34;✅ Unauthorized access: PASSED\u0026#34;) def run_tests(): \u0026#34;\u0026#34;\u0026#34;Run all tests\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Starting backend validation tests...\\n\u0026#34;) token = get_token() print(\u0026#34;✅ Authentication: PASSED\\n\u0026#34;) test_unauthorized() test_create_item(token) test_get_items(token) print(\u0026#34;\\n🎉 All tests passed!\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: run_tests() Run the test:\npython test_backend.py Always run comprehensive tests before deploying to production. Include positive tests (expected behavior), negative tests (error handling), and load tests (performance).\nImages Required: api-test-results.png - curl command results load-test.png - Artillery load test results cloudwatch-logs.png - CloudWatch logs view test-results.png - Automated test script output "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/4-eventparticipated/4.5-event5/",
	"title": "Event 5",
	"tags": [],
	"description": "",
	"content": "AWS Well-Architected Security Pillar Workshop Event Information Date \u0026amp; Time: Saturday, November 29, 2025, 8:30 AM – 12:00 PM\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent Purpose This morning workshop provided a comprehensive deep-dive into the AWS Well-Architected Security Pillar, covering all five security domains: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response.\nThe session was designed to equip participants with practical knowledge on implementing security best practices in AWS environments, with real-world examples from Vietnamese enterprises.\nAgenda 8:30 – 8:50 AM | Opening \u0026amp; Security Foundation Security Pillar in Well-Architected Framework\nRole of Security Pillar in the Well-Architected Framework Core principles: Least Privilege – Zero Trust – Defense in Depth AWS Shared Responsibility Model Top cloud security threats in Vietnam Pillar 1 — Identity \u0026amp; Access Management 8:50 – 9:30 AM | Modern IAM Architecture\nIAM fundamentals: Users, Roles, Policies – avoiding long-term credentials IAM Identity Center: SSO, permission sets SCP \u0026amp; Permission Boundaries for multi-account environments MFA, credential rotation, Access Analyzer Mini Demo: Validate IAM Policy + simulate access Pillar 2 — Detection 9:30 – 9:55 AM | Detection \u0026amp; Continuous Monitoring\nCloudTrail (org-level), GuardDuty, Security Hub Logging at every layer: VPC Flow Logs, ALB/S3 logs Alerting \u0026amp; automation with EventBridge Detection-as-Code (infrastructure + rules) 9:55 – 10:10 AM | Coffee Break Pillar 3 — Infrastructure Protection 10:10 – 10:40 AM | Network \u0026amp; Workload Security\nVPC segmentation, private vs public placement Security Groups vs NACLs: application models WAF + Shield + Network Firewall Workload protection: EC2, ECS/EKS security basics Pillar 4 — Data Protection 10:40 – 11:10 AM | Encryption, Keys \u0026amp; Secrets\nKMS: key policies, grants, rotation Encryption at-rest \u0026amp; in-transit: S3, EBS, RDS, DynamoDB Secrets Manager \u0026amp; Parameter Store — rotation patterns Data classification \u0026amp; access guardrails Pillar 5 — Incident Response 11:10 – 11:40 AM | IR Playbook \u0026amp; Automation\nIR lifecycle according to AWS framework Playbooks: Compromised IAM key S3 public exposure EC2 malware detection Snapshot, isolation, evidence collection Auto-response with Lambda/Step Functions 11:40 – 12:00 PM | Wrap-Up \u0026amp; Q\u0026amp;A Summary of 5 security pillars Common pitfalls \u0026amp; real-world practices in Vietnamese enterprises Security learning roadmap (Security Specialty, SA Pro certifications) Key Learnings Least Privilege is fundamental – always start with minimal permissions and expand as needed Zero Trust architecture assumes no implicit trust, even within the network Defense in Depth requires security controls at multiple layers Detection capabilities must be automated and continuous Incident response playbooks should be documented and tested regularly Application to My Work Implement IAM Access Analyzer in current projects Set up GuardDuty and Security Hub for centralized security monitoring Create incident response playbooks for common scenarios Review and tighten Security Group rules using least privilege Enable encryption for all data stores (S3, RDS, DynamoDB) Personal Experience This workshop was incredibly valuable for understanding AWS security holistically:\nThe practical demos made abstract concepts tangible Learning about common security pitfalls in Vietnamese enterprises was eye-opening The incident response playbooks provided actionable templates Understanding the relationship between all five pillars helps in designing comprehensive security architectures Takeaways Security is a continuous journey, not a destination Automation is key to maintaining security at scale The Well-Architected Security Pillar provides a comprehensive framework for cloud security Regular security reviews and improvements are essential AWS provides powerful native tools for each security domain Event Photos "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives (per CloudJourney roadmap) Server migration: Using AWS MGN and Server Migration Service (Module 2). Application migration patterns: Lift-and-shift to EC2. Hands-on replication and cutover. Update cost management with migrated resources. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 2 — Server migration: Study AWS MGN setup, replication servers, test migrations. 06/10/2025 06/10/2025 https://cloudjourney.awsstudygroup.com/ - Migrate to AWS Tuesday Labs: Launch MGN replication agent, migrate sample VM to EC2, validate post-launch. 07/10/2025 07/10/2025 https://cloudjourney.awsstudygroup.com/ - Module 2 Labs Wednesday Application lift-and-shift: Package app with SMS, deploy to EC2, configure dependencies. 08/10/2025 08/10/2025 https://cloudjourney.awsstudygroup.com/ - Migrate to AWS Thursday Cutover practice: Perform test cutover, DNS updates, rollback simulation. 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ - Module 2 Friday Cost review: Analyze migrated resources with Cost Explorer, apply savings plans. 10/10/2025 10/10/2025 https://cloudjourney.awsstudygroup.com/ - Cost Management Results \u0026amp; Achievements (Week 5) Successful VM migration: MGN replicated Windows/Linux servers to EC2, tested applications post-cutover. Lift-and-shift completed: Legacy app running on EC2 with minimal changes, dependencies resolved. Rollback procedures documented: Dry-run cutover with traffic switchback in under 5 minutes. Cost optimizations applied: Reserved Instances purchased for steady-state EC2 fleet. Mini-project reviewed: End-to-end migration of a sample workload from \u0026ldquo;on-prem\u0026rdquo; to AWS. Issues Encountered \u0026amp; Mitigations MGN agent connectivity issues → configured outbound rules in on-prem firewall simulation. App dependencies failed post-migration → used SSM for automated patching. Cutover DNS propagation delay → implemented Route 53 health checks for faster failover. Next Steps (Week 6) Database migration with AWS DMS. Hybrid network connectivity: Direct Connect/VPN. Post-migration validation and optimization. Prepare for optimization module. "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/5-workshop/5.5-policy/",
	"title": "Monitoring &amp; Observability",
	"tags": [],
	"description": "",
	"content": "Monitoring \u0026amp; Observability In this section, you will set up monitoring and alerting for your FastAPI Lambda application using CloudWatch and SNS.\nObservability Components Component AWS Service Purpose Logs CloudWatch Logs Store and query application logs Metrics CloudWatch Metrics Lambda invocations, errors, duration Alarms CloudWatch Alarms Alert on error thresholds Notifications SNS Send alerts via email/SMS Terraform Observability Module This is the actual observability module from the project:\n# infra/modules/observability/main.tf variable \u0026#34;project_name\u0026#34; {} variable \u0026#34;lambda_name\u0026#34; {} resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;lambda\u0026#34; { name = \u0026#34;/aws/lambda/${var.lambda_name}\u0026#34; retention_in_days = 14 } resource \u0026#34;aws_sns_topic\u0026#34; \u0026#34;alerts\u0026#34; { name = \u0026#34;${var.project_name}-alerts\u0026#34; } resource \u0026#34;aws_cloudwatch_metric_alarm\u0026#34; \u0026#34;lambda_errors\u0026#34; { alarm_name = \u0026#34;${var.project_name}-5xx\u0026#34; comparison_operator = \u0026#34;GreaterThanThreshold\u0026#34; evaluation_periods = 1 metric_name = \u0026#34;Errors\u0026#34; namespace = \u0026#34;AWS/Lambda\u0026#34; period = 60 statistic = \u0026#34;Sum\u0026#34; threshold = 1 dimensions = { FunctionName = var.lambda_name } alarm_actions = [aws_sns_topic.alerts.arn] } output \u0026#34;alerts_topic_arn\u0026#34; { value = aws_sns_topic.alerts.arn } Module Usage in main.tf # infra/main.tf module \u0026#34;observability\u0026#34; { source = \u0026#34;./modules/observability\u0026#34; project_name = var.project_name lambda_name = module.lambda.lambda_name } CloudWatch Log Group Configuration The log group is automatically created for Lambda with:\nRetention: 14 days (configurable) Log stream: Created per Lambda execution environment Lambda Error Alarm The alarm triggers when:\nMetric: Lambda Errors Threshold: \u0026gt; 1 error Period: 60 seconds (1 minute) Evaluation: 1 period SNS Alert Topic Configure email subscription for alerts:\n# Subscribe email to SNS topic aws sns subscribe \\ --topic-arn arn:aws:sns:us-east-1:ACCOUNT_ID:fastapi-lambda-alerts \\ --protocol email \\ --notification-endpoint your-email@example.com Application Logging Configuration The application uses structured logging for CloudWatch:\n# backend/app/core/logging.py import logging import json import sys class JSONFormatter(logging.Formatter): def format(self, record): log_obj = { \u0026#34;timestamp\u0026#34;: self.formatTime(record), \u0026#34;level\u0026#34;: record.levelname, \u0026#34;message\u0026#34;: record.getMessage(), \u0026#34;module\u0026#34;: record.module, \u0026#34;function\u0026#34;: record.funcName, } if record.exc_info: log_obj[\u0026#34;exception\u0026#34;] = self.formatException(record.exc_info) return json.dumps(log_obj) def configure_logging(): handler = logging.StreamHandler(sys.stdout) handler.setFormatter(JSONFormatter()) root = logging.getLogger() root.setLevel(logging.INFO) root.handlers = [handler] Correlation ID Middleware Track requests across logs with correlation IDs:\n# backend/app/api/middleware.py import uuid import logging from starlette.middleware.base import BaseHTTPMiddleware from starlette.requests import Request logger = logging.getLogger(__name__) class CorrelationMiddleware(BaseHTTPMiddleware): async def dispatch(self, request: Request, call_next): correlation_id = request.headers.get(\u0026#34;X-Correlation-ID\u0026#34;, str(uuid.uuid4())) logger.info(f\u0026#34;Request started\u0026#34;, extra={ \u0026#34;correlation_id\u0026#34;: correlation_id, \u0026#34;method\u0026#34;: request.method, \u0026#34;path\u0026#34;: request.url.path }) response = await call_next(request) response.headers[\u0026#34;X-Correlation-ID\u0026#34;] = correlation_id logger.info(f\u0026#34;Request completed\u0026#34;, extra={ \u0026#34;correlation_id\u0026#34;: correlation_id, \u0026#34;status_code\u0026#34;: response.status_code }) return response def add_correlation_middleware(app): app.add_middleware(CorrelationMiddleware) CloudWatch Insights Queries Useful queries for analyzing logs:\n-- Find all errors in last 24 hours fields @timestamp, @message | filter @message like /ERROR/ | sort @timestamp desc | limit 100 -- Request latency analysis fields @timestamp, @duration | stats avg(@duration) as avg_duration, max(@duration) as max_duration, count() as request_count | by bin(1h) -- Top error messages fields @message | filter level = \u0026#34;ERROR\u0026#34; | stats count() by message | sort count desc | limit 10 Creating Additional Alarms Add more alarms for better observability:\n# High latency alarm resource \u0026#34;aws_cloudwatch_metric_alarm\u0026#34; \u0026#34;lambda_duration\u0026#34; { alarm_name = \u0026#34;${var.project_name}-high-latency\u0026#34; comparison_operator = \u0026#34;GreaterThanThreshold\u0026#34; evaluation_periods = 3 metric_name = \u0026#34;Duration\u0026#34; namespace = \u0026#34;AWS/Lambda\u0026#34; period = 300 statistic = \u0026#34;Average\u0026#34; threshold = 5000 # 5 seconds dimensions = { FunctionName = var.lambda_name } alarm_actions = [aws_sns_topic.alerts.arn] } # Throttling alarm resource \u0026#34;aws_cloudwatch_metric_alarm\u0026#34; \u0026#34;lambda_throttles\u0026#34; { alarm_name = \u0026#34;${var.project_name}-throttles\u0026#34; comparison_operator = \u0026#34;GreaterThanThreshold\u0026#34; evaluation_periods = 1 metric_name = \u0026#34;Throttles\u0026#34; namespace = \u0026#34;AWS/Lambda\u0026#34; period = 60 statistic = \u0026#34;Sum\u0026#34; threshold = 1 dimensions = { FunctionName = var.lambda_name } alarm_actions = [aws_sns_topic.alerts.arn] } Viewing Logs in AWS Console Navigate to CloudWatch → Log groups Select /aws/lambda/fastapi-lambda-fn Browse log streams for specific invocations Viewing Metrics Navigate to CloudWatch → Metrics Select Lambda → By Function Name Choose your function and select metrics: Invocations Errors Duration ConcurrentExecutions Use CloudWatch Logs Insights for advanced log analysis. The JSON-formatted logs make it easy to filter and aggregate data.\nImages Required: monitoring-architecture.png - Monitoring flow diagram cloudwatch-logs.png - CloudWatch log group view cloudwatch-metrics.png - Lambda metrics dashboard sns-subscription.png - SNS email subscription alarm-triggered.png - Alarm in ALARM state "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Building a FastAPI Backend on AWS Lambda with DevSecOps Overview This workshop guides you through building a complete FastAPI Backend API running on AWS Lambda (container-based) with automated CI/CD pipeline, integrated security scanning, and Infrastructure as Code using Terraform.\nYou will learn how to:\nBuild a FastAPI application with layered architecture (API → Service → Repository) Package FastAPI as a Lambda container image using Mangum adapter Set up CI/CD pipeline with GitLab CI and AWS CodePipeline/CodeBuild Integrate security scanning with Semgrep (SAST) and Trivy (vulnerability scanning) Deploy infrastructure using Terraform modules Store data in DynamoDB tables (products, orders, users) Implement JWT authentication with Secrets Manager Set up monitoring with CloudWatch and SNS alerts Architecture The architecture follows this request flow:\nClient Request ↓ API Gateway (HTTP API) ↓ Lambda Function (Container) ↓ Mangum (ASGI Adapter) ↓ FastAPI Application ├── /auth → Authentication (JWT) ├── /products → Product CRUD └── /orders → Order CRUD ↓ DynamoDB Tables ├── products (product_id, name, price, stock, category) ├── orders (order_id, user_id, products, status) └── users (user_id, email, password_hash) Project Structure devsecops-aws/ ├── backend/ # FastAPI Application │ ├── app/ │ │ ├── api/routers/ # API endpoints (auth, products, orders) │ │ ├── services/ # Business logic layer │ │ ├── repositories/ # DynamoDB data access │ │ ├── models/ # Pydantic models │ │ ├── core/ # Config, security, logging │ │ ├── main.py # FastAPI app entry point │ │ └── lambda_handler.py # Lambda handler (Mangum) │ ├── Dockerfile # Lambda container image │ ├── requirements.txt │ ├── semgrep.yml # Security scanning rules │ └── trivyignore.txt ├── infra/ # Terraform Infrastructure │ ├── main.tf # Main orchestration │ ├── variables.tf │ ├── modules/ │ │ ├── dynamodb/ # DynamoDB tables │ │ ├── iam/ # IAM roles \u0026amp; policies │ │ ├── lambda_container/ # Lambda function │ │ ├── apigw/ # API Gateway HTTP API │ │ ├── observability/ # CloudWatch \u0026amp; SNS │ │ └── route53/ # Custom domain (optional) ├── pipeline/ # AWS CodePipeline config │ ├── buildspec-build.yml │ ├── buildspec-deploy.yml │ └── codepipeline.tf └── .gitlab-ci.yml # GitLab CI/CD Content Workshop Overview Prerequisites Setting up CI/CD Pipeline Building FastAPI Backend Monitoring \u0026amp; Observability Clean up Resources AWS Services Used Category Services Compute AWS Lambda (Container Image) API API Gateway HTTP API (v2) Database Amazon DynamoDB Security AWS Secrets Manager, IAM CI/CD AWS CodePipeline, CodeBuild, ECR Monitoring Amazon CloudWatch, SNS DNS Amazon Route 53 (optional) IaC Terraform Estimated Time \u0026amp; Cost Item Details Duration 3-4 hours Level Intermediate Cost ~$5-10 (if cleaned up after workshop) "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at [AWS CLOUD] from [Sept 9] to [Dec 30], I had the valuable opportunity to learn, practice, and apply my specialized knowledge in a professional real-world environment.\nTogether with my team, I built a CI/CD pipeline for the project [Secure Serverless for Global Applications]. Through this process, I significantly improved my skills in [Python Programming, Data Analysis, Technical Report Writing, etc.].\nRegarding work ethic, I always strived to complete assigned tasks well, strictly adhered to company regulations, and proactively collaborated with colleagues to ensure the team\u0026rsquo;s overall progress.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, application of knowledge, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Highlights Beyond completing assigned tasks, I achieved several positive results:\nAhead-of-schedule Completion: Completed the module/feature [20] days earlier than expected with high stability. Rapid Technology Adaptation: Quickly mastered [Gitlab] within the short initial period of the internship, despite not having studied it deeply at university. Process Improvement: Proposed an idea to optimize the [CI/CD] workflow, helping to save processing time for the team. Needs Improvement Although I have put in my best effort, I recognize there are areas I need to refine to become more professional:\nComplex Problem-Solving: I need to develop a more multi-dimensional and calm approach when facing difficult bugs or unexpected situations. Professional Communication: I aim to be more confident and concise when presenting technical ideas in professional meetings or before a group. Time Management: I need to learn how to better prioritize tasks during high-pressure phases of the project to minimize stress. Next Steps Based on the lessons learned from this internship, I have built a self-development plan for the near future:\nTechnical Specialization: Continue to dive deep into [AWS Cloud services] to understand core concepts and best coding practices. Application to Capstone Project: Apply professional workflows (such as Agile/Scrum) and source code management standards learned at the company to my upcoming Graduation Thesis. Language Proficiency: Improve my ability to read professional English documentation to update myself with the latest technology trends quickly. "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives (per CloudJourney roadmap) Database migration: Schema conversion and data transfer with DMS (Module 2). Hybrid networking: VPN/Direct Connect setup. Post-migration tasks: Testing, optimization, cleanup. Risk mitigation strategies. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 2 — DB migration: Study DMS endpoints, replication instances, ongoing replication. 13/10/2025 13/10/2025 https://cloudjourney.awsstudygroup.com/ - Migrate to AWS Tuesday Labs: Migrate MySQL to RDS PostgreSQL using DMS, validate data integrity. 14/10/2025 14/10/2025 https://cloudjourney.awsstudygroup.com/ - Module 2 Labs Wednesday Hybrid networking: Create Client VPN endpoint, connect EC2 to \u0026ldquo;on-prem\u0026rdquo; VPC. 15/10/2025 15/10/2025 https://cloudjourney.awsstudygroup.com/ - Migrate to AWS Thursday Post-migration: Performance tuning, security scans, application testing. 16/10/2025 16/10/2025 https://cloudjourney.awsstudygroup.com/ - Module 2 Friday Risk management: Document rollback plans, simulate failures, review compliance. 17/10/2025 17/10/2025 https://cloudjourney.awsstudygroup.com/ - Risk Mitigation Results \u0026amp; Achievements (Week 6) DB migrated seamlessly: DMS full load + CDC from on-prem MySQL to RDS, zero data loss verified. Hybrid connectivity established: VPN allowing secure access between VPCs, latency under 50ms. Post-migration optimized: Tuned RDS parameters, scanned for vulnerabilities with Inspector. Risk framework built: Rollback playbook created, failure scenarios tested with Chaos Engineering basics. Module 2 foundational migration complete, ready for optimization. Issues Encountered \u0026amp; Mitigations DMS schema conversion errors → used SCT tool for pre-conversion fixes. VPN auth failures → enabled SAML integration for user access. Post-migration query slowdown → indexed tables and enabled query caching. Next Steps (Week 7) Enter Module 3: Focus on cost optimization and performance efficiency. Implement auto-scaling and right-sizing. Begin security pillar with GuardDuty. Analyze overall journey costs so far. "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/5-workshop/5.6-cleanup/",
	"title": "Clean up Resources",
	"tags": [],
	"description": "",
	"content": "Clean up Resources Complete this section to avoid unexpected charges. AWS resources incur costs while running.\nResources Created in This Workshop Resource Service Created By Lambda Function AWS Lambda Terraform API Gateway HTTP API API Gateway Terraform DynamoDB Tables (3) DynamoDB Terraform ECR Repository ECR Manual/Pipeline CloudWatch Log Group CloudWatch Terraform SNS Topic SNS Terraform CloudWatch Alarms CloudWatch Terraform IAM Role \u0026amp; Policies IAM Terraform CodePipeline CodePipeline Terraform (optional) CodeBuild Projects CodeBuild Terraform (optional) Option 1: Terraform Destroy (Recommended) If you used Terraform to create resources:\n# Navigate to infra directory cd infra # Review what will be destroyed terraform plan -destroy # Destroy all resources terraform destroy -auto-approve Option 2: Manual Cleanup Step 1: Delete Lambda Function aws lambda delete-function --function-name fastapi-lambda-fn Step 2: Delete API Gateway # Get API ID API_ID=$(aws apigatewayv2 get-apis --query \u0026#34;Items[?Name==\u0026#39;fastapi-lambda-http\u0026#39;].ApiId\u0026#34; --output text) # Delete API aws apigatewayv2 delete-api --api-id $API_ID Step 3: Delete DynamoDB Tables # Delete all three tables aws dynamodb delete-table --table-name products aws dynamodb delete-table --table-name orders aws dynamodb delete-table --table-name users Step 4: Delete ECR Repository # Force delete with all images aws ecr delete-repository --repository-name fastapi-lambda --force Step 5: Delete CloudWatch Resources # Delete log group aws logs delete-log-group --log-group-name /aws/lambda/fastapi-lambda-fn # Delete alarms aws cloudwatch delete-alarms --alarm-names fastapi-lambda-5xx fastapi-lambda-high-latency fastapi-lambda-throttles Step 6: Delete SNS Topic # Get topic ARN TOPIC_ARN=$(aws sns list-topics --query \u0026#34;Topics[?contains(TopicArn,\u0026#39;fastapi-lambda-alerts\u0026#39;)].TopicArn\u0026#34; --output text) # Delete topic aws sns delete-topic --topic-arn $TOPIC_ARN Step 7: Delete IAM Role # Detach policies first aws iam list-attached-role-policies --role-name fastapi-lambda-role --query \u0026#34;AttachedPolicies[].PolicyArn\u0026#34; --output text | xargs -n1 aws iam detach-role-policy --role-name fastapi-lambda-role --policy-arn # Delete inline policies aws iam list-role-policies --role-name fastapi-lambda-role --query \u0026#34;PolicyNames\u0026#34; --output text | xargs -n1 aws iam delete-role-policy --role-name fastapi-lambda-role --policy-name # Delete role aws iam delete-role --role-name fastapi-lambda-role Step 8: Delete CodePipeline (if created) aws codepipeline delete-pipeline --name fastapi-lambda-pipeline Step 9: Delete CodeBuild Projects (if created) aws codebuild delete-project --name fastapi-lambda-build aws codebuild delete-project --name fastapi-lambda-deploy Verification After cleanup, verify no resources remain:\n# Check Lambda aws lambda list-functions --query \u0026#34;Functions[?starts_with(FunctionName, \u0026#39;fastapi-lambda\u0026#39;)]\u0026#34; # Check DynamoDB aws dynamodb list-tables --query \u0026#34;TableNames[?contains(@, \u0026#39;products\u0026#39;) || contains(@, \u0026#39;orders\u0026#39;) || contains(@, \u0026#39;users\u0026#39;)]\u0026#34; # Check ECR aws ecr describe-repositories --query \u0026#34;repositories[?starts_with(repositoryName, \u0026#39;fastapi-lambda\u0026#39;)]\u0026#34; # Check API Gateway aws apigatewayv2 get-apis --query \u0026#34;Items[?starts_with(Name, \u0026#39;fastapi-lambda\u0026#39;)]\u0026#34; Cost Verification Navigate to AWS Cost Explorer Set date range to include workshop period Verify no ongoing charges Summary Congratulations! You have completed the FastAPI on AWS Lambda workshop.\nWhat you learned:\n✅ Build FastAPI applications with layered architecture ✅ Package Python apps as Lambda container images ✅ Set up CI/CD with CodePipeline and security scanning ✅ Deploy infrastructure with Terraform modules ✅ Implement JWT authentication ✅ Configure monitoring with CloudWatch and SNS Next Steps Add more API endpoints (PUT, DELETE for orders) Implement pagination for list endpoints Add caching with ElastiCache Set up custom domain with Route 53 Implement rate limiting Add integration tests to the pipeline Images Required: terraform-destroy.png - Terraform destroy output resources-deleted.png - Verification of deleted resources "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives (per CloudJourney roadmap) Cost optimization: Right-sizing, Compute Optimizer (Module 3). Performance efficiency: Auto-scaling, caching refinements. Initial security: GuardDuty threat detection. Operational excellence basics: Automation with SSM. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 3 — Cost opt: Analyze usage with Cost Explorer, recommendations from Compute Optimizer. 20/10/2025 20/10/2025 https://cloudjourney.awsstudygroup.com/ - Optimize Tuesday Labs: Right-size EC2 instances, purchase Savings Plans, set up budgets. 21/10/2025 21/10/2025 https://cloudjourney.awsstudygroup.com/ - Module 3 Labs Wednesday Performance: Refine ASG policies, implement ElastiCache for app caching. 22/10/2025 22/10/2025 https://cloudjourney.awsstudygroup.com/ - Optimize Thursday Security intro: Enable GuardDuty, review findings, integrate with EventBridge. 23/10/2025 23/10/2025 https://cloudjourney.awsstudygroup.com/ - Module 3 Friday Automation: Use SSM for patching EC2 fleet, document runbooks. 24/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ - Operational Excellence Results \u0026amp; Achievements (Week 7) Costs reduced by 25%: EC2 right-sized, Savings Plans applied, anomaly detection alerts set. Performance improved: ASG responding in \u0026lt;1min, caching hitting 80% success rate. Threat detection active: GuardDuty scanning logs, simulated threats triaged. Automation streamlined: SSM automating weekly patches, runbooks in Git repo. Mid-journey cost analysis: Total spend tracked, optimizations yielding savings. Issues Encountered \u0026amp; Mitigations Compute Optimizer data lag → waited 48h for full recommendations. GuardDuty false positives → tuned suppression rules based on environment. SSM execution failures → granted additional IAM roles to instances. Next Steps (Week 8) Continue Module 3: Reliability with multi-AZ/DR, deeper security. Fault tolerance patterns. Prepare for modernization in Week 9. Team sharing session on optimizations. "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " âš ï¸ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I donâ€™t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives (per CloudJourney roadmap) Reliability pillar: High availability, disaster recovery (Module 3). Advanced security: Zero-trust, compliance with Config. Incident response basics. Cross-pillar integration. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 3 — Reliability: Design multi-AZ architectures, RTO/RPO planning. 27/10/2025 27/10/2025 https://cloudjourney.awsstudygroup.com/ - Optimize Tuesday Labs: Set up DR with Pilot Light strategy, test failover to secondary region. 28/10/2025 28/10/2025 https://cloudjourney.awsstudygroup.com/ - Module 3 Labs Wednesday Security advanced: Implement zero-trust with IAM policies, enable AWS Config rules. 29/10/2025 29/10/2025 https://cloudjourney.awsstudygroup.com/ - Optimize Thursday Compliance: Audit resources with Config, remediate non-compliant items. 30/10/2025 30/10/2025 https://cloudjourney.awsstudygroup.com/ - Module 3 Friday Incident response: Simulate outage, use Incident Manager, document lessons. 31/10/2025 31/10/2025 https://cloudjourney.awsstudygroup.com/ - Operational Excellence Results \u0026amp; Achievements (Week 8) HA/DR resilient: Multi-AZ RDS/ECS clusters, Pilot Light DR tested with \u0026lt;15min RTO. Zero-trust enforced: Least-privilege IAM, Config conformance packs applied. Compliance score 95%: Automated remediation for common rules like encryption. IR playbook refined: Outage simulation resolved in 10min, integrated with PagerDuty. Optimizations integrated: Security/reliability enhancing prior workloads. Issues Encountered \u0026amp; Mitigations DR cross-region latency → optimized with global Accelerator. Config rule evaluation delays → increased evaluation frequency. IR simulation alert fatigue → refined notification thresholds. Next Steps (Week 9) Transition to Module 4: Microservices decomposition. Serverless patterns with Lambda/EventBridge. API design with API Gateway/AppSync. Modernization team demo. Note: All labs and content follow the CloudJourney curriculum at https://cloudjourney.awsstudygroup.com/. Ensure screenshots, logs, and scripts are stored in the project repository for verification.\n"
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives (per CloudJourney roadmap) Microservices: Bounded contexts, decomposition of monolith (Module 4). Serverless: Event-driven with Lambda, Step Functions. API-first: REST/GraphQL design. DevOps integration. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 4 — Microservices: Study domain-driven design, decompose sample app into services. 03/11/2025 03/11/2025 https://cloudjourney.awsstudygroup.com/ - Modernize Tuesday Labs: Refactor monolith to ECS microservices, use API Gateway for routing. 04/11/2025 04/11/2025 https://cloudjourney.awsstudygroup.com/ - Module 4 Labs Wednesday Serverless: Build workflow with Lambda and EventBridge rules. 05/11/2025 05/11/2025 https://cloudjourney.awsstudygroup.com/ - Modernize Thursday APIs: Create REST API with Gateway, GraphQL with AppSync. 06/11/2025 06/11/2025 https://cloudjourney.awsstudygroup.com/ - Module 4 Friday DevOps: Set up CI/CD with CodePipeline for microservices deployment. 07/11/2025 07/11/2025 https://cloudjourney.awsstudygroup.com/ - DevOps Results \u0026amp; Achievements (Week 9) Monolith decomposed: 3 bounded services running independently on ECS, inter-service comms via SQS. Event-driven app: Lambda orchestrating order processing, triggered by EventBridge. APIs live: REST endpoints secured with Cognito, GraphQL schema with resolvers. CI/CD automated: CodePipeline deploying updates, integrated with GitHub. Modernization demo: End-to-end refactored app showcased to team. Issues Encountered \u0026amp; Mitigations Service discovery challenges → integrated Service Discovery with ECS. EventBridge rule matching issues → debugged with CloudWatch Logs Insights. Pipeline approval gates slow → automated for non-prod environments. Next Steps (Week 10) Module 5: Container orchestration with ECS/EKS. Fargate for serverless containers. Service mesh with App Mesh. Security in containers. Note: All labs and content follow the CloudJourney curriculum at https://cloudjourney.awsstudygroup.com/. Ensure screenshots, logs, and scripts are stored in the project repository for verification.\n"
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives (per CloudJourney roadmap) Container orchestration: Deep dive into ECS and EKS (Module 5). Serverless containers with Fargate. Networking and scaling in Kubernetes. Container security best practices. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 5 — ECS advanced: Cluster networking, service discovery, blue-green deployments. 10/11/2025 10/11/2025 https://cloudjourney.awsstudygroup.com/ - Containers Tuesday Labs: Deploy multi-container app on ECS, integrate with ALB and RDS. 11/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ - Module 5 Labs Wednesday EKS intro: Set up managed node groups, deploy Helm charts. 12/11/2025 12/11/2025 https://cloudjourney.awsstudygroup.com/ - Containers Thursday Fargate: Run EKS pods on Fargate, test scaling. 13/11/2025 13/11/2025 https://cloudjourney.awsstudygroup.com/ - Module 5 Friday Security: Image scanning with ECR, runtime protection with GuardDuty. 14/11/2025 14/11/2025 https://cloudjourney.awsstudygroup.com/ - Container Security Results \u0026amp; Achievements (Week 10) ECS production-ready: Blue-green deployment with zero downtime, service mesh via App Mesh. EKS cluster operational: Kubernetes workloads deployed, autoscaling HPA configured. Fargate efficiency: Serverless pods reducing management overhead, cost 20% lower. Secure pipeline: Scanned images blocking vulnerabilities, GuardDuty alerts for anomalies. Containerized modernization: Previous microservices migrated to EKS. Issues Encountered \u0026amp; Mitigations EKS pod scheduling failures → adjusted node taints and affinities. Fargate networking limits → used VPC CNI for pod IPs. Scanning false negatives → updated ECR policies for latest scans. Next Steps (Week 11) Module 6: Data lake with S3/Lake Formation. ETL pipelines with Glue. Real-time analytics with Kinesis. Governance and visualization. Note: All labs and content follow the CloudJourney curriculum at https://cloudjourney.awsstudygroup.com/. Ensure screenshots, logs, and scripts are stored in the project repository for verification.\n"
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives (per CloudJourney roadmap) Data lake architecture: S3 partitioning, Lake Formation governance (Module 6). ETL/ELT: Glue jobs for data processing. Streaming: Kinesis for real-time ingestion. BI basics with QuickSight. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 6 — Data lake: Design S3-based lake, enable versioning, set up Lake Formation catalogs. 17/11/2025 17/11/2025 https://cloudjourney.awsstudygroup.com/ - Data \u0026amp; Analytics Tuesday Labs: Partition data in S3, grant permissions via Lake Formation. 18/11/2025 18/11/2025 https://cloudjourney.awsstudygroup.com/ - Module 6 Labs Wednesday ETL: Create Glue crawlers and jobs, transform CSV to Parquet. 19/11/2025 19/11/2025 https://cloudjourney.awsstudygroup.com/ - Data \u0026amp; Analytics Thursday Streaming: Set up Kinesis Data Streams, produce/consume events. 20/11/2025 20/11/2025 https://cloudjourney.awsstudygroup.com/ - Module 6 Friday Visualization: Connect QuickSight to S3/Glue, build dashboards. 21/11/2025 21/11/2025 https://cloudjourney.awsstudygroup.com/ - BI Results \u0026amp; Achievements (Week 11) Data lake built: S3 organized with Athena queries, Lake Formation securing access. ETL automated: Glue jobs processing 1M rows, output to S3 optimized formats. Real-time pipeline: Kinesis streaming app logs, analytics with Kinesis Analytics. Dashboards interactive: QuickSight visualizing ETL outputs, shared with team. Data journey complete: From ingestion to insights, integrated with prior services. Issues Encountered \u0026amp; Mitigations Lake Formation permission propagation delays → used blueprints for faster setup. Glue job memory errors → scaled DPU allocation. Kinesis shard limits → monitored throughput and resharded. Next Steps (Week 12) Module 7: Intro to AI/ML with Bedrock, SageMaker basics. Generative AI applications. Capstone project review. Final reflections and workforce program prep. Note: All labs and content follow the CloudJourney curriculum at https://cloudjourney.awsstudygroup.com/. Ensure screenshots, logs, and scripts are stored in the project repository for verification.\n"
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives (per CloudJourney roadmap) AI/ML foundations: Bedrock for foundation models, SageMaker for custom ML (Module 7). Build simple RAG app. Responsible AI practices. Capstone integration and review. Tasks This Week (by day) Day Task Start Date Completion Date Reference Monday Module 7 — AI intro: Explore Bedrock models, invoke LLMs for text generation. 24/11/2025 24/11/2025 https://cloudjourney.awsstudygroup.com/ - AI/ML Tuesday Labs: Build RAG pipeline with Bedrock and Kendra for document Q\u0026amp;A. 25/11/2025 25/11/2025 https://cloudjourney.awsstudygroup.com/ - Module 7 Labs Wednesday Custom ML: SageMaker Studio setup, train simple model on S3 data. 26/11/2025 26/11/2025 https://cloudjourney.awsstudygroup.com/ - AI/ML Thursday Responsible AI: Bias detection with Clarify, explainability tools. 27/11/2025 27/11/2025 https://cloudjourney.awsstudygroup.com/ - Module 7 Friday Capstone: Integrate AI into data pipeline, full journey review, prepare submission. 28/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ - Review Results \u0026amp; Achievements (Week 12) Generative AI app: Bedrock-powered chatbot querying S3 docs via RAG, accuracy \u0026gt;85%. ML model deployed: SageMaker endpoint serving predictions, integrated with Lambda. Ethical practices applied: Models audited for bias, explanations added to outputs. Capstone delivered: End-to-end app combining migration, optimization, containers, data, AI. 12-week journey complete: All modules covered, portfolio built, ready for FCJ Workforce. Issues Encountered \u0026amp; Mitigations Bedrock token limits → chunked inputs for longer queries. SageMaker training costs → used spot instances for non-urgent jobs. Bias in dataset → augmented training data for balance. Next Steps (Post-Program) Apply to FCJ Workforce Program. Contribute to community projects. Pursue AWS certifications (Solutions Architect Associate). Mentor next cohort on learnings. Note: All labs and content follow the CloudJourney curriculum at https://cloudjourney.awsstudygroup.com/. Ensure screenshots, logs, and scripts are stored in the project repository for verification.\n"
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/aws-report-fcj/en/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]